{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计分析-回归分析与分类分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 概述与大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 从建模的目的看回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归分析与分类分析都是一种基于统计模型的统计分析方法。它们都研究因变量（被解释变量）与自变量（解释变量）之间存在的潜在关系，并通过统计模型的形式将这些潜在关系进行显式的表达。不同的是，回归分析中因变量是连续变量，如工资、销售额；而分类分析中因变量是属性变量，如判断邮件“是or否”为垃圾邮件。\n",
    "\n",
    "上一段我们提到，回归是一种基于统计模型的分析方法，因此回归分析的过程本质上一种建模过程。统计建模的主要任务有二：预测与推断。\n",
    "\n",
    "所谓预测，就是利用一个训练完毕的模型$\\hat{f}$，根据输入的自变量$X$获得对应的输出$Y$。在预测任务中，如果模型$\\hat{f}$可以准确地提供预测，那么$\\hat{f}$是什么形式并不重要，而如果$\\hat{f}$的形式非常复杂且难以解释，我们可以将之称为黑盒模型(Black Box)。举一个例子，假设$X_1,X_2,\\cdots ,X_p$是某个病人的血样特征，$Y$测量了病人使用药物后出现严重不良反应的风险，那么如果存在一个模型可以很好地通过$X$以预测$Y$，那自然是再好不过的事了。此时，模型的形式、变量之间的关系在正确预测面前都显得不那么重要。事实上，当前具有强大预测性能的模型大多都是黑盒模型，如强大的Xgboost机器学习算法以及各种深度学习算法，它们的模型可解释性差，我们难以解释其中一些参数的含义与统计性质。\n",
    "\n",
    "与预测相对应的另一任务便是推断。在很多情况下，我们对当$X_1,X_2,\\cdots ,X_p$变化时**如何影响**$Y$更感兴趣，此时，我们估计模型$\\hat{f}$的目的不是为了预测$Y$，而是想明白两者之间的关系，更深层次地讲，我们想要知道模型内各种参数的数值与统计推断性质等等。在这种情况下，模型的可解释性就非常重要了，而通常我们在推断任务中最常使用的模型正是线性回归模型。举一个例子，在研究各因素对商品销售量的场景中，我们会更关注以下问题：哪类媒体对销量有直接的贡献？增加电视广告费用能对销售量带来多少程度的增加？等等，这就是典型的推断问题。\n",
    "\n",
    "弄清楚了预测与推断的区别，我们重新审视一下回归分析：回归分析更加注重对因变量与自变量之间潜在关系的推断，所使用的统计模型也相对简单（一般为线性模型），如果你在比赛中需要分析各变量间的潜在相关关系，便可以考虑使用回归分析。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 课程大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本轮课程，我们将先后学习回归分析与分类分析的知识。其中，回归分析中我们主要学习经典线性模型(CLM, Classical Linear Model)与最小二乘估计(OLS)，广义线性模型(GLM)只做简单介绍；分类分析中我们主要学习基于线性模型的Logistics模型与Probit模型。具体大纲如下：\n",
    "\n",
    "· 回归的思想与线性回归模型介绍\n",
    "\n",
    "· OLS估计在经典线性回归模型假设下的统计推断\n",
    "\n",
    "· 线性回归模型中的参数检验\n",
    "\n",
    "· 线性回归模型设定的误差分析\n",
    "\n",
    "· 异方差下回归建模的解决方法\n",
    "\n",
    "首先，我们将了解回归的基本思想，并对最常用的回归模型-经典线性模型的模型形式、模型假设（也称为CLM假设）做基本的介绍。\n",
    "\n",
    "知晓了模型的形式后，下一步自然是进行模型参数的估计，并推断估计参数的统计性质。在线性模型中，这些参数就是每个自变量的系数，我们想知道：使用何种方法进行参数估计呢？参数在这种估计方法下能否接近真实参数呢？估计的误差有多大呢？我们将学习使用最小二乘法(OLS)对线性模型进行估计，并探究OLS估计下各参数的统计性质。可以告诉大家，在满足CLM假设的前提下，OLS估计是经典线性模型最优的参数估计法；基于CLM假设与OLS估计，我们便可以对模型进行各种假设检验，包括参数显著性检验，模型显著性检验等等。\n",
    "\n",
    "然而，理想很丰满，现实很骨感，我们所获得的实验数据不总是能满足CLM假设中的每一条假设。某个假设不成立会给模型参数估计的准确性（无偏性）、稳定性（方差）以及假设检验带来多少影响呢？这就是模型设定误差分析需要研究的内容。最后，若数据不满足CLM假设中的某个假设，需要找到对应的解决办法，在本轮课程，我们重点探讨不满足同方差假设下参数估计方法与参数检验方法的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 回归模型总述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一章节，我们将对“回归”进行宏观的介绍，使大家对回归有直观的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 回归思想与一般回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 横截面数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横截面数据是回归分析最主要的分析数据类型，它可以视为在**同一时间点（或抽样时间差异可以被忽略）**上对**多个抽样个体**的观测数据。通常，我们记第$i$个个体的观测数据为$(x_i,y_i)$。如果以抽样时间点与抽样个体数目为维度划分数据类型，除了横截面数据外，还有时间序列数据以及面板数据。时间序列数据为单个个体在不同时间点上的观测数据，而面板数据则是多个个体在不同时间点上的观测数据。对时间序列数据的分析需要用到时间序列分析的知识，对面板数据的分析则是高级计量经济学的内容，在本次课程我们不对它们做介绍。三者的区别如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"./images/横截面数据.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 回归思想——条件均值建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横截面数据最重要的一个特征，就是我们可以将采集的数据$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$近似视为来自一个潜在总体的随机样本，即假设\n",
    "$$\n",
    "\\left(x_{1}, y_{1}\\right), \\cdots,\\left(x_{n}, y_{n}\\right) \\sim^{i i d}(x, y)\n",
    "$$\n",
    "我们进行数据分析的最终目的是为了找到$x$与$y$之间的关系并用模型显性表示出来，此时最理想的状态是使用一个**条件分布**刻画$x$对$y$的影响\n",
    "$$\n",
    "F_{y \\mid x}\n",
    "$$\n",
    "即在任意给定$x$的条件下都有一个明确的分布$F$刻画$y$的状态。但是在实际问题中，直接估计这个条件分布几乎是一件不可能的事，且我们也难以对分布进行解释与应用。于是，我们退而求其次通过分布的一般数字特征对两者的关系进行推断，如条件分布的中心位置，形状，即考虑**条件均值、条件方差**\n",
    "$$\n",
    "E(y \\mid x), \\operatorname{Var}(y \\mid x)\n",
    "$$\n",
    "而回归正是利用条件均值$E(y \\mid x)$来刻画$x$与$y$的关系，回归建模的本质也正是“条件均值的建模”。那么，怎么理解条件均值建模呢？我们举一个不典型的例子帮助大家理解。\n",
    "\n",
    "假设某个样本量为100的数据集中，自变量$x$有1,2,3,4,5五个值，样本的因变量$y$都来自以其自变量为均值，方差为1的正态分布。我们想要刻画因变量与自变量之间的变化关系，就要找出可以代表各种类样本内（在此例中以自变量为划分依据）共性的特征，用这些特征来描绘变化关系。最直观也是最简单的特征就是条件均值，即给定$x$的条件下样本的均值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'E(Y|X)')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=list()\n",
    "y=list()\n",
    "for i in [1,2,3,4,5]:\n",
    "    y_norm=stats.norm.rvs(i,1,20,random_state=i).tolist()\n",
    "    y.extend(y_norm)\n",
    "    x1=np.ones(20)*i\n",
    "    x1=x1.tolist()\n",
    "    x.extend(x1)\n",
    "\n",
    "data={'x':x,'y':y}\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "sns.regplot(x='x',y='y',data=df)\n",
    "plt.title('E(Y|X)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上图我们可以看出，$E(y|x=1)=1$,$E(y|x=2)=2$,…,$E(y|x=x_0)=x_0$。通过条件均值，我们可以推断出$x$与$y$的关系可以用模型$y=x+u$来刻画，其中，$u$被称为随机误差，可理解为：除$x$外，其他影响$y$取值的因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 一般回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述例子中，我们使用模型$y=x+u$刻画了$x$与$y$的关系，这说明了在这个数据集中我们将模型设定为了\n",
    "$$\n",
    "y=x+u\n",
    "$$\n",
    "事实上，如果我们将上述公式中的$x$泛化成条件均值$E(y|x)$，那么我们就能得到最一般的回归模型\n",
    "$$\n",
    "y=E(y|x)+u\n",
    "$$\n",
    "这也就意味着，所谓回归模型的建模，**本质上就是条件均值建模**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 回归模型的条件解读**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般回归模型$y=E(y|x)+u$暗含了一个天然成立的假设：\n",
    "1. 随机误差的条件期望$E(u|x)=0$\n",
    "\n",
    "利用重期望公式，我们可以根据假设1进一步推得下面两个推论:\n",
    "\n",
    "推论1. 随机误差的无条件期望$E(u)=0$——这表示其他因素对$y$的平均影响为0\n",
    "<br>\n",
    "推论2. 随机误差$u$与自变量$x$协方差$Cov(u,x)=0$——这表示其他因素与参与回归的$x$不相关！\n",
    "\n",
    "根据假设1，我们可以将一般回归模型表示成一种新的形式：\n",
    "$$\n",
    "y=E(y|x)+u\\Longleftrightarrow y=m\\left( x \\right) +u, where\\,\\,E\\left( u|x \\right) =0\n",
    "$$\n",
    "在这里，$E(u|x)=0$等价于$m(x)=E(y|x)$。事实上，用这一种形式表示回归模型更常见，也更有利于接下来对模型$m(x)$具体形式的假定，因为这告诉了我们：只要假定随机误差$u$与$x$不相关（这里可理解为其他影响$y$的外生因素与内生因素$x$不相关），我们就可以根据需要假定回归模型的具体形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 线性模型形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的学习中我们介绍了回归模型的一般形式。在实际建模中，为了有效的估计，我们必须对模型中$m(x)$的形式进行具体的假定。在所有模型假定形式中，线性回归模型是最常用假定形式，也是回归分析中最重要的模型，是本次课程重点讲解的内容。\n",
    "\n",
    "线性模型假设有：\n",
    "$$\n",
    "m(x)=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{p} x_{p}\n",
    "$$\n",
    "于是，线性回归模型可表示为：\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{p} x_{p}+u, \\quad E\\left(u \\mid x_{1}, \\cdots, x_{p}\\right)=0\n",
    "$$\n",
    "回归分析主要研究如何有效地估计模型中的参数$\\hat{\\beta}_i$，并利用模型进行推断与预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 从简单线性回归到多元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 用简单线性回归理解对模型的解释**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为大家快速理解线性回归模型，我们先假设$x$是一维的，即只考虑一个因素对$y$的影响，此时亦称模型为简单线性回归，形式为\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x+u, \\quad E(u \\mid x)=0\n",
    "$$\n",
    "$\\beta_{0}$是截距项，可以理解为$x=0$时$y$的期望值，一般情况下，如果我们回归的任务是推断，则截距通常不重要；\n",
    "<br>\n",
    "$\\beta_{1}=\\frac{\\Delta m(x)}{\\Delta x}$，可理解为$x$每增加一个单位，$y$**平均**增加$\\beta_1$个单位。\n",
    "\n",
    "此后，我们将默认模型含有$E(u|x)=0$的设定（因为只有这样模型才代表回归模型），该条件不再以书面形式写出。\n",
    "\n",
    "我们举一个例子帮助大家理解：\n",
    "\n",
    "**Example1.** 假设大学成绩colGPA与大学测验水平ACT间关系为\n",
    "$$\n",
    "\\text { colGPA }=\\beta_{0}+\\beta_{1} \\text { hsGPA }+u\n",
    "$$\n",
    "$\\beta_1$系数的解释为：每增加1单位大学测验水平，大学成绩会增加$\\beta_1$个单位；由于该模型中自变量只有高中成绩，而大学成绩水平肯定还受其他因素影响，因此该模型中的随机误差包含了如学习时长、自主学习能力等因素。\n",
    "\n",
    "注意：设定$E(u|x)=0$的存在暗含了**在该模型中**高中测验成绩、自主学习能力等因素与自变量大学测验水平无关，但这在**实际问题中**未必成立。而一旦它们存在相关性，就意味着模型假设不符合实际情况，模型估计的有效性与准确性也将受到影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 进行全面的回归建模——多元线性回归**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单纯的简单线性模型有很大的局限性，原因有二：\n",
    "\n",
    "1、在实际问题中，因变量$y$通常受多个因素影响，这些因素之间可能彼此之间存在线性相关性（后续的学习中我们将这种现象称为多重共线性），而默认假设$E(u|x)=0$的直接推论(推论2)就是其他影响因素与$x$线性无关，显然不一定符合实际情况。\n",
    "\n",
    "2、如果我们想推断一个变量对另一个变量的因果关系，就要保持尽可能多的其他因素的不变，因此需要尽量把关键因素纳入到回归模型当中，这样便可以控制多个变量，查看某个特定变量变化对自变量的影响。\n",
    "\n",
    "因此在实际问题中，我们更多地使用多元线性回归。一般的多元线性回归模型可写成：\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "$u$依旧为随机误差项，它表示除$x_1$,…,$x_k$以外的其他因素对因变量$y$的影响，且同样满足假设\n",
    "$$\n",
    "E\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=0\n",
    "$$\n",
    "$\\beta_i=\\frac{\\partial m\\left( x \\right)}{\\partial x_i}$是回归函数对变量$x_i$的偏导数，它被解释为**在保持其他自变量不变的情况下，$x_i$每增加一单位，$y$平均增加$\\beta_i$个单位**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· Example2.** 依旧以上面的大学成绩例子为例，这一次我们增加一个高中成绩hsGPA变量，此时模型变为\n",
    "$$\n",
    "\\mathrm{colGPA}=\\beta _0+\\beta _1\\mathrm{hsGPA}+\\beta _2\\mathrm{ACT}+u\n",
    "$$\n",
    "在模型增加了一个我们认为非常重要的变量后，模型的估计会产生怎样的变化呢？我们使用python对该例的数据集进行回归分析，比较两种模型的区别。具体的python实现过程我们将稍后介绍，大家只需要关注这里的结果即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>ACT</th>\n",
       "      <th>hsGPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     const   ACT  hsGPA\n",
       "0      1.0  21.0    3.0\n",
       "1      1.0  24.0    3.2\n",
       "2      1.0  26.0    3.6\n",
       "3      1.0  27.0    3.5\n",
       "4      1.0  28.0    3.9\n",
       "..     ...   ...    ...\n",
       "136    1.0  23.0    3.3\n",
       "137    1.0  25.0    3.6\n",
       "138    1.0  21.0    3.4\n",
       "139    1.0  26.0    3.7\n",
       "140    1.0  28.0    3.3\n",
       "\n",
       "[141 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载数据\n",
    "gpa1=pd.read_stata('./data/gpa1.dta')\n",
    "\n",
    "# 在数据集中提取自变量\n",
    "X1=gpa1.ACT\n",
    "X2=gpa1[['ACT','hsGPA']]\n",
    "# 提取因变量\n",
    "y=gpa1.colGPA\n",
    "\n",
    "# 为自变量增添截距项\n",
    "X1=sm.add_constant(X1)\n",
    "X2=sm.add_constant(X2)\n",
    "display(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>2.402979</td>\n",
       "      <td>8.798591e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.027064</td>\n",
       "      <td>1.389927e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params        pvalue\n",
       "const  2.402979  8.798591e-16\n",
       "ACT    0.027064  1.389927e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.286328</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.009426</td>\n",
       "      <td>0.383297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsGPA</th>\n",
       "      <td>0.453456</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params    pvalue\n",
       "const  1.286328  0.000238\n",
       "ACT    0.009426  0.383297\n",
       "hsGPA  0.453456  0.000005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 拟合两个模型\n",
    "gpa_lm1=sm.OLS(y,X1).fit()\n",
    "gpa_lm2=sm.OLS(y,X2).fit()\n",
    "\n",
    "# 输出两个模型的系数与对应p值\n",
    "p1=pd.DataFrame(gpa_lm1.pvalues,columns=['pvalue'])\n",
    "c1=pd.DataFrame(gpa_lm1.params,columns=['params'])\n",
    "p2=pd.DataFrame(gpa_lm2.pvalues,columns=['pvalue'])\n",
    "c2=pd.DataFrame(gpa_lm2.params,columns=['params'])\n",
    "display(c1.join(p1,how='right'))\n",
    "display(c2.join(p2,how='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，变量ACT在两个回归模型中的系数并不一致，且其在单独回归时变量显著，但增添了变量hsGPA后变得不显著。这说明多个变量共同回归绝不等同于多个变量各自进行单变量回归，且在后面的课程中我们会知道将多个重要变量都纳入回归模型的重要性。总之，大家在此只需要知道：**多元线性回归非常重要，后续的学习也将围绕多元线性回归展开！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型系数的估计方法——OLS估计及其性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一章，我们了解了回归的本质——条件均值建模、介绍了最经典的回归模型——（多元）线性回归模型的形式、参数解释与一些注意事项。那么接下来有一个非常自然而然的问题摆在我们面前——你这个线性回归模型里的参数是使用什么方法计算出来的呢？按照你这种方法计算出来的参数是否可靠呢？它们又具备哪些统计性质呢？那么这一章，我们将学习线性回归中最常用、最经典的系数估计方法——普通最小二乘估计法(Ordinary Least Squares, OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 OLS估计的思想与原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 OLS估计的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一个关于储蓄与收入间关系的例子解释ols估计的思想。将储蓄savings视作因变量$y$，将收入income视作自变量$x$，由于只有一个自变量，因此可用简单线性回归模型假设两者关系为$y=\\beta_{0}+\\beta_{1} x+u$，即一条带有趋势与截距的直线。那么，这条直线应该“长成”怎样才算是一条“好的直线”呢？直观上看，最佳的拟合直线应该尽可能的贴合样本点，如下图所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/ols.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直线尽可能贴合样本点，意味着在所有直线当中，我们要选出一条离所有样本点距离的总和最小的直线。那么，这个距离该如何衡量？我们将模型回归参数分别记为$\\hat{\\beta}_{0}$，$\\hat{\\beta}_{1}$，并定义$\\hat{y}_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}$为样本在自变量为$x_i$下的拟合值，记样本实际观测值$y_i$与拟合值$\\hat{y}_{i}$之差为拟合残差$\n",
    "\\hat{u}_{i}=y_{i}-\\hat{y}_{i}$。\n",
    "\n",
    "不同的距离定义方法是不同估计法的一大区别，OLS对距离的定义是：残差的平方${\\hat{u}_i}^2$。因此OLS估计的思想是：**OLS估计求得的系数$\\hat{\\beta}_{0}$、$\\hat{\\beta}_{1}$，将使直线与所有样本的拟合残差的平方和最小**，即\n",
    "$$\n",
    "\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}\\right)=\\operatorname{argmin} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i}\\right)^{2}\n",
    "$$\n",
    "对于多元线性回归，OLS估计的思想也完全相同，只不过多元线性回归的模型不是一条直线，而是一个多维的超平面。对于多元线性回归的OLS估计目标函数，有\n",
    "$$\n",
    "\\left( \\hat{\\beta}_0,\\cdots ,\\hat{\\beta}_k \\right) =\\mathrm{arg}\\min \\sum_{i=1}^n{\\left( y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1i}-\\hat{\\beta}_kx_{ki} \\right) ^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 OLS估计的求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 从优化角度看OLS求解**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在上面知晓了需要求解的函数后，接下来就要开始进行求解了。\n",
    "\n",
    "记目标函数为\n",
    "$$\n",
    "Q\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)^{2}\n",
    "$$\n",
    "这是一个以$(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k})$作为未知变量的多元函数，我们要求得最小值点，可以令各元偏导数等于0，构建一个$k+1$维的方程组求解：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)=0 \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i 1}=0 \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i 2}=0 \\\\\n",
    "&\\cdots \\quad \\cdots \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i k}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "以上方程组中，每个方程有$k+1$个自变量，且有$k+1$个方程，根据线性代数的知识，我们可以求得$(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k})$的唯一解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS求解的矩阵表示**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述思路证明了OLS估计求解系数的可行性，但是我们还不知道系数估计的具体形式，接下来我们就利用矩阵的形式给出OLS求解的结果。求解过程无需掌握，但大家需要大致了解每个数学符号的含义与指代，在后续的理论介绍时，它们会被反复提及。\n",
    "\n",
    "由于我们有$n$个样本，因此根据模型有以下$n$个等式成立\n",
    "$$\n",
    "y_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\cdots+\\beta_{k} x_{i k}+u_{i}, \\quad i=1, \\cdots, n\n",
    "$$\n",
    "将它们联立称方程组，并表示成矩阵形式\n",
    "$$\n",
    "\\boldsymbol{y}=\\boldsymbol{X\\beta }+\\boldsymbol{u}\n",
    "$$\n",
    "这里，$\\boldsymbol{y}=\\left( y_1,y_2,\\cdots ,y_n \\right) ^{'},\\quad \\boldsymbol{\\beta} =\\left( \\beta _0,\\beta _1,\\cdots ,\\beta _k \\right) ^{'},\\quad \\boldsymbol{u}=\\left( u_1,u_2,\\cdots ,u_n \\right) ^{'}$。\n",
    "<br>\n",
    "并记：$x_{i}^{\\prime}=\\left(1, x_{i 1}, x_{i 2}, \\cdots, x_{i k}\\right), \\boldsymbol{X}=\\left(x_{1}^{\\prime}, x_{2}^{\\prime}, \\cdots, x_{n}^{\\prime}\\right)^{\\prime}$，值得注意的是，$\\boldsymbol{X}$是一个$n\\times \\left( k+1 \\right) $维的矩阵，n为样本个数，k为自变量个数，它也被称为设计阵。\n",
    "\n",
    "以上是真实模型的矩阵表示形式，对于我们实际拟合的模型及其残差，其矩阵形式则为\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}}=\\boldsymbol{X\\hat{\\beta}},\\quad \\boldsymbol{\\hat{u}}=\\boldsymbol{y}-\\boldsymbol{\\hat{y}}\n",
    "$$\n",
    "根据令残差平方和偏导数为0的思想，有\n",
    "$$\n",
    "Q(\\hat{\\beta})=\\sum_{i=1}^{n} \\hat{u}_{i}^{2}=\\hat{u}^{\\prime} \\hat{u}=(y-X \\hat{\\beta})^{\\prime}(y-X \\hat{\\beta})=y^{\\prime} y-2 \\hat{\\beta}^{\\prime} X^{\\prime} y+\\hat{\\beta}^{\\prime} X^{\\prime} X \\hat{\\beta}\n",
    "$$\n",
    "运用向量求导的知识得\n",
    "$$\n",
    "X^{\\prime} X \\hat{\\beta}=X^{\\prime} y \\Rightarrow \\hat{\\beta}=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} y\n",
    "$$\n",
    "至此，我们就得到了各系数估计向量$\\hat{\\beta}$的矩阵表达式了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的系数向量为：\n",
      "[1.28632777 0.00942601 0.45345589]\n",
      "-----------------------------------\n",
      "软件计算的系数为：\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.286328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.009426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsGPA</th>\n",
       "      <td>0.453456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params\n",
       "const  1.286328\n",
       "ACT    0.009426\n",
       "hsGPA  0.453456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 以gpa_lm2为例\n",
    "## 手动计算系数的估计向量\n",
    "X2_T=X2.values.T\n",
    "X_inv=np.linalg.inv(np.dot(X2_T,X2)) # 求矩阵乘积的逆矩阵\n",
    "Xy=np.dot(X2_T,y.values)\n",
    "beta_hat=np.dot(X_inv,Xy)\n",
    "print('手动计算的系数向量为：')\n",
    "print(beta_hat)\n",
    "\n",
    "## 软件计算的系数向量\n",
    "print('-----------------------------------')\n",
    "print('软件计算的系数为：')\n",
    "display(c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 拟合优度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多元线性模型\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "我们使用OLS得到了一个拟合模型\n",
    "$$\n",
    "\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1x_1+\\cdots +\\hat{\\beta}_kx_k\n",
    "$$\n",
    "一个很自然的问题是：这个模型对数据的拟合效果如何？这个问题可以进一步引申为：模型中的自变量$x_i$在多大程度上解释了$y$的变异？（$y$的趋势变化可以理解为是一种带有规律性的变异）\n",
    "\n",
    "在探讨这个问题前，我们先引入几个简单而又重要的概念。\n",
    "\n",
    "· TSS(Total sum of squares)，总平方和\n",
    "$$\n",
    "T S S=\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\n",
    "$$\n",
    "\n",
    "· ESS(Explained sum of squares)，解释平方和\n",
    "$$\n",
    "E S S=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}\n",
    "$$\n",
    "\n",
    "· RSS(Resiual sum of squares)，残差平方和\n",
    "$$\n",
    "R S S=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n",
    "$$\n",
    "直观上RSS是一个可以度量拟合优度的量，因为残差平方和越小，意味着预测值与真实值之间的差距越小。但是RSS的大小没有一个标准，它将随着样本量的增大而增大，因此单纯的RSS不是一个合格的衡量拟合优度的量。\n",
    "\n",
    "这个时候我们可以从另一个角度去理解回归建模的意义。我们之所以想构建模型，是因为想找到**造成$y$值变化**的因素，模型解释的变异占总变异的比例越多，这个模型的解释力度就越大，模型的拟合优度也就越好。我们举一个简单的例子：某天，一个村子的菜包子涨了1块钱，大家都想知道究竟是什么原因导致这1块钱的涨幅。小红和小明综合了当天所有发生变化的外因素（其实就是自变量啦~），分别构建了两个模型将这些外因素的变化和菜包子涨价的1块钱联系在一起。在小红的模型预测下，这些外因素变化会使菜包子涨价0.99块钱，而小明的模型则只预测到了0.1块钱的涨价。我们认为，小红的模型解释1块钱涨价中的0.99块，而小明只解释了0.1块，因此小红的模型更优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 回归拟合优度——R方**\n",
    "\n",
    "理解了用“解释变异的比例”衡量回归模型拟合优度的思想，构造拟合优度就有思路了。回归分析中最常用的拟合优度是R方，定义为\n",
    "$$\n",
    "R^{2}=\\frac{E S S}{T S S}\n",
    "$$\n",
    "其中，TSS度量了因变量$y$的总样本变异，而ESS度量了模型拟合值$\\hat{y}$的总变异，也就是解释了的变异。事实上三种平方和存在关系$TSS=RSS+ESS$（大家可以尝试自己推导），这说明：总变异可以被拆分为解释了变异和未被解释的变异，残差平方和度量了“剩余信息”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的R方为：0.1764216724583183\n",
      "-------------------------------------------------\n",
      "软件计算的R方为：0.17642159463450557\n"
     ]
    }
   ],
   "source": [
    "# 动手计算模型gpa_lm2的R方\n",
    "TSS_gpa=np.sum(np.power(gpa1.colGPA-np.mean(gpa1.colGPA),2))\n",
    "RSS_gpa=np.sum(np.power(gpa_lm2.resid,2))\n",
    "gpa_lm2_R2=1-RSS_gpa/TSS_gpa\n",
    "print('手动计算的R方为：{}'.format(gpa_lm2_R2))\n",
    "print('-------------------------------------------------')\n",
    "# 直接输出模型gpa_lm2的R方\n",
    "gpa_lm2_R2=gpa_lm2.rsquared\n",
    "print('软件计算的R方为：{}'.format(gpa_lm2_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两者结果十分接近，之所以不完全相同可能是numpy计算与statsmodels计算存在小差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 OLS估计的代数性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用OLS估计对线性回归模型进行参数估计，估计出来的模型将有许多重要的特性与性质。其中有的性质是OLS估计自身求解过程所带来的，我们称之为代数性质，这部分性质是天然成立的；而有的性质只有在某些特定的模型假设下才能成立，一旦实际数据违反了假设，这些性质将不再成立。\n",
    "\n",
    "在这一小节，我们将简单学习OLS估计的代数性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 代数性质**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS估计的代数性质来自于其本身求解过程中的方程组。我们观察一下上面的方程组，可以很快地总结出以下两条公式\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t&\\sum_{i=1}^n{\\left( y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}-\\cdots -\\hat{\\beta}_kx_{ik} \\right)}=\\sum_{i=1}^n{\\hat{u}_i}=0\\\\\n",
    "\t&\\sum_{i=1}^n{\\left( y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}-\\cdots -\\hat{\\beta}_kx_{ij} \\right)}x_{ij}=\\sum_{i=1}^n{\\hat{u}_i}x_{ij}=0, j=1,\\cdots ,k\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "第一条公式意味着：OLS估计预测残差之和为0；此外，这可以推出预测残差的均值也为0，即$\\bar{\\hat{u}}=0$。我们以之前的gpa1回归建模为例，看看模型在python中实际计算出来的残差之和是否为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "简单回归模型的残差和（保留四位小数点）：-0.0000\n",
      "多元回归模型的残差和（保留四位小数点）：-0.0000\n"
     ]
    }
   ],
   "source": [
    "print('简单回归模型的残差和（保留四位小数点）：{:.4f}'.format(sum(gpa_lm1.resid)))\n",
    "print('多元回归模型的残差和（保留四位小数点）：{:.4f}'.format(sum(gpa_lm2.resid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二条公式可以进行以下恒等推导：\n",
    "$$\n",
    "\\sum_{i=1}^n{x_{ik}}\\hat{u}_i=\\sum_{i=1}^n{x_{ik}}\\left( \\hat{u}_i-\\bar{\\hat{u}} \\right) =\\sum_{i=1}^n{\\left( x_{ik}-\\bar{x} \\right)}\\left( \\hat{u}_i-\\bar{\\hat{u}} \\right) =Cov\\left( x_k,\\hat{u} \\right) =0, j=1,\\cdots ,k\n",
    "$$\n",
    "这是OLS估计最重要的代数性质，它意味着OLS估计的残差与参与回归的自变量不相关。这预示着：如果我们消除因变量$y$与某些自变量$x_j$之间的线性相关性，可以先进行线性回归然后取残差！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回归模型中残差与自变量hsGPA的样本协方差为（保留四位小数点）：-0.0000\n"
     ]
    }
   ],
   "source": [
    "# 定义计算残差的计算函数\n",
    "from pylab import *\n",
    "def de_mean(x):\n",
    "    xmean = np.mean(x)\n",
    "    return [xi - xmean for xi in x]\n",
    "\n",
    "# 定义计算样本协方差的计算函数\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x), de_mean(y)) / (n-1)\n",
    "\n",
    "print('回归模型中残差与自变量hsGPA的样本协方差为（保留四位小数点）：{:.4f}'.format(covariance(gpa_lm2.resid,gpa1.hsGPA)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 经典线性模型假设下OLS估计的性质 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 经典线性模型假设-CLM假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLM假设总共有六条，在六条假设下OLS估计具有非常优良的性质，接下来让我们看看这六条假设是什么，每条意味着什么。\n",
    "\n",
    "**· MLR.1 总体模型假设** \n",
    "\n",
    "总体模型可以写为\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "该假设假定了我们正确地判断了因变量和自变量之间的关系——既正确设定了模型形式为上述的线性形式，又正确纳入了所有自变量。\n",
    "\n",
    "**· MLR.2 随机误差条件均值零假设**\n",
    "\n",
    "随机误差$u$满足\n",
    "$$\n",
    "E\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=0\n",
    "$$\n",
    "我们在前面的小节提到过这一假设的推论，它意味着所有非自变量的其他因素都与自变量线性无关。\n",
    "\n",
    "**· MLR.3 随机抽样假设**\n",
    "\n",
    "$n$个来自上述总体的样本均为随机抽样样本，彼此之间相互独立\n",
    "\n",
    "**· MLR.4 非完全共线性假设**\n",
    "\n",
    "这些样本的所有自变量间不能存在有完全共线性，即不能存在某一自变量可由其余自变量进行线性表示的情况。数学语言为：不存在不全为零的$a_{0}, a_{1}, \\cdots, a_{k}$使得\n",
    "$$\n",
    "a_{0}+a_{1} x_{i 1}+a_{2} x_{i 2}+\\cdots+a_{k} x_{i k}=0, \\forall i=1, \\cdots, n\n",
    "$$\n",
    "**· MLR.5 同方差假设**\n",
    "\n",
    "随机误差$u$的条件方差恒为一个常数，即\n",
    "$$\n",
    "\\operatorname{Var}\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}\n",
    "$$\n",
    "根据条件方差的性质，上述等式可等价为(大家可以想想为什么会这样呢？)\n",
    "$$\n",
    "\\operatorname{Var}\\left(y \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}\n",
    "$$\n",
    "同方差假设看起来有一点点抽象，但其实它非常好理解也非常直观——数据的波动程度不受自变量影响，不论$x_i$如何变化，数据与样本条件均值的偏离程度都是恒定的。我们看看以下两张对比图，直观地感受同方差与异方差的区别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/同方差.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着$X$的增大，左图样本数据间的距离越来越大，这是方差越来越大的体现；而右图样本数据间的距离则相对稳定，这说明它们的方差相对稳定。\n",
    "\n",
    "**· MLR.6 正态性假设**\n",
    "\n",
    "该假设假定随机误差$u$在任何自变量$x$已知的条件下服从正态分布\n",
    "$$\n",
    "u \\mid x \\sim N\\left(0, \\sigma^{2}\\right)\n",
    "$$\n",
    "这一假设实际上是MLR.2与MLR.5假设的升级版，即在随机误差$u$的零条件期望与恒定条件方差的基础上，增加了一个服从条件正态分布的假设。\n",
    "\n",
    "以上六个假设是一种非常严格、理想化的假设，只有在这些假设成立的基础上我们才能对OLS估计在线性回归模型上的性质作进一步的研究。当然，实际的数据并不一定都能满足这些假设，有关样本数据是否可以满足这些假设的识别检验、不满足假设的后果以及改进方案，我们将在以后的章节学习。接下来，对于一个样本数据，请大家默认其满足CLM假设，我们将见识到OLS系数估计法在CLM假设下的优越性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 OLS估计的性质-最优的线性无偏估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们之所以对样本数据进行建模并估计模型的系数，是因为我们认为存在一个潜在的、**正确的**模型（函数）可以描述这些数据的特征。在回归任务中，这个我们假定正确的函数被称为**总体回归函数**\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "而我们使用各种模型估计方法得出的对这个总体回归函数的函数则被称为**样本回归函数**\n",
    "$$\n",
    "\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1x_1+\\cdots +\\hat{\\beta}_kx_k\n",
    "$$\n",
    "对于一个样本回归函数而言，怎样子的函数才算是好函数呢？答案当然是，各估计系数$\\hat{\\beta}$都尽可能接近真实系数$\\beta$；并且使用同一总体的不同取样样本进行估计时，估计出来的系数越稳定越好。而OLS估计在这两方面的表现都非常不错。\n",
    "\n",
    "**· OLS系数估计的无偏性**\n",
    "\n",
    "**定理1.** 在CLM假设**MLR.1-MLR.4**下，$\\hat{\\beta}$是$\\beta$的无偏估计，即\n",
    "$$\n",
    "E\\left(\\hat{\\beta}_{j}\\right)=\\beta_{j}, \\forall j=0,1, \\cdots, k\n",
    "$$\n",
    "证明见附录\n",
    "\n",
    "无偏性意味着我们使用OLS进行多次试验后，估计出来的系数均值与参数的真实值是吻合的，这是一件激励人心的事，这说明我们估计出来的系数非常接近真实系数！接下来我们看看估计系数的稳定性——方差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS系数估计的方差**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先看一下CLM假设下，OLS估计系数的**方差与标准误**具体为多少。\n",
    "\n",
    "**· 定理2.** 在CLM假设**MLR.1-MLR.5**（增加了同方差假设MLR.5)下，$\\hat{\\beta}$的方差-协方差矩阵表达式为\n",
    "$$\n",
    "\\operatorname{Cov}(\\hat{\\beta})=\\sigma^{2}\\left(X^{\\prime} X\\right)^{-1}\n",
    "$$\n",
    "其中，$X$为设计阵；$\\sigma^{2}$为同方差假设$\\operatorname{Var}\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}$中的随机误差的方差。\n",
    "\n",
    "由于$\\hat{\\beta}$向量的协方差矩阵对角线就是每个系数的方差，因此有OLS估计的方差以及标准差(SD,Standard Deviation)\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)=\\sigma^{2}\\left(X^{\\prime} X\\right)_{j+1, j+1}^{-1}\n",
    "$$\n",
    "$$\n",
    "\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)=\\sigma \\sqrt{\\left(X^{\\prime} X\\right)_{j+1, j+1}^{-1}}\n",
    "$$\n",
    "我们已经知道了各估计系数$\\hat{\\beta}_{j}$的方差表达式了，但是还有最后一个问题需要解决：$\\sigma^{2}$是我们假设的一个参数，实际我们并不知道它是多少，因此我们要给出它的估计。\n",
    "\n",
    "既然$\\sigma^{2}$是随机误差的方差，那么它的估计形式总体上应该遵循样本方差的形式，即有这样的形式\n",
    "$$\n",
    "\\hat{\\sigma}^2=\\frac{1}{df}\\sum_{i=1}^n{\\left( \\hat{u} \\right) ^2}\n",
    "$$\n",
    "其中，$df$是自由度，它一般是样本个数与待估计参数个数的差。而对于随机误差的估计形式$\\hat{u}$，我们回想一下随机误差的含义：它是样本真值$y$与总体回归函数$m(x)$的偏差，其构造是不是与残差十分相似呢？因此在对随机误差的估计中，我们就用残差代替随机误差，故有\n",
    "$$\n",
    "\\hat{\\sigma}^{2}=\\frac{1}{n-k-1} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)^{2}=\\frac{RSS}{n-k-1}\n",
    "$$\n",
    "$\\hat{\\sigma}$被称为回归标准误(standard error of regression)，我们将$\\hat{\\sigma}$带入到上述的标准差$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$当中，得到的结果被称为估计系数的标准误(standard error)\n",
    "$$\n",
    "\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)=\\hat{\\sigma} \\sqrt{\\left(X^{\\prime} X\\right)_{j+1, j+1}^{-1}}\n",
    "$$\n",
    "在这里我们需要明确地指出，因为$\\sigma$的未知性，估计系数标准差$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$是在实际中无法得知的，python软件也不会输出这个指标。只有估计系数标准误$\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)$才是明确的！\n",
    "\n",
    "**· 定理3.** 在CLM假设**MLR.1-MLR.5**下，$\\hat{\\sigma}^{2}$是$\\sigma^{2}$的无偏估计，即\n",
    "$$\n",
    "E\\left(\\hat{\\sigma}^{2}\\right)=\\sigma^{2}\n",
    "$$\n",
    "这个定理表明，在CLM假设下，我们上述对随机误差的方差的估计是“准确的”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的回归标准误：0.1158148144382463\n",
      "软件计算的回归标准误：0.1158148144382463\n",
      "-------------------------------------------------\n",
      "手动计算的ACT系数标准误：0.010777187759672815\n",
      "软件计算的ACT系数标准误：0.010777187759672789\n"
     ]
    }
   ],
   "source": [
    "# 依旧以gpa_lm2模型为例\n",
    "# 手动计算标准误，并比较python直接输出的结果\n",
    "\n",
    "# 回归标准误\n",
    "## 手动计算\n",
    "df=gpa_lm2.df_resid # 计算自由度\n",
    "sigma=RSS_gpa/df\n",
    "print('手动计算的回归标准误：{}'.format(sigma))\n",
    "## 软件输出\n",
    "sigma2=gpa_lm2.scale\n",
    "print('软件计算的回归标准误：{}'.format(sigma2))\n",
    "print('-------------------------------------------------')\n",
    "\n",
    "# 变量ACT系数的标准误\n",
    "## 手动计算\n",
    "X2_T=X2.values.T\n",
    "X_inv=np.linalg.inv(np.dot(X2_T,X2)) # 求矩阵乘积的逆矩阵\n",
    "se_beta1=np.sqrt(sigma*X_inv[(1,1)])\n",
    "print('手动计算的ACT系数标准误：{}'.format(se_beta1))\n",
    "\n",
    "## 软件输出\n",
    "se_beta1=gpa_lm2.bse[1]\n",
    "print('软件计算的ACT系数标准误：{}'.format(se_beta1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以直接使用接口summary来直观的展示模型拟合的各种指标结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 colGPA   R-squared:                       0.176\n",
      "Model:                            OLS   Adj. R-squared:                  0.164\n",
      "Method:                 Least Squares   F-statistic:                     14.78\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           1.53e-06\n",
      "Time:                        15:37:15   Log-Likelihood:                -46.573\n",
      "No. Observations:                 141   AIC:                             99.15\n",
      "Df Residuals:                     138   BIC:                             108.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.2863      0.341      3.774      0.000       0.612       1.960\n",
      "ACT            0.0094      0.011      0.875      0.383      -0.012       0.031\n",
      "hsGPA          0.4535      0.096      4.733      0.000       0.264       0.643\n",
      "==============================================================================\n",
      "Omnibus:                        3.056   Durbin-Watson:                   1.885\n",
      "Prob(Omnibus):                  0.217   Jarque-Bera (JB):                2.469\n",
      "Skew:                           0.199   Prob(JB):                        0.291\n",
      "Kurtosis:                       2.488   Cond. No.                         298.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(gpa_lm2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在前面使用代码实现的各种指标，大部分都可以在summary汇总表格中找到。如R-squared就是R方，Df Residuals就是模型自由度；估计系数部分，第一列是系数的估计值，第二列是估计系数的标准误，第三第四列则是我们在下一章节介绍的系数显著性指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS系数估计的最优线性无偏性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1-MLR.4下，我们知道了OLS估计是无偏的；在MLR.1-MLR.5下，我们得出了OLS估计方差的表达式。那么在这里我们要告诉大家，在所有无偏估计当中，OLS估计是最优的，因为有如下定理\n",
    "\n",
    "**· Gauss-Markov定理.** 在CLM假设**MLR.1-MLR.5**下，在$\\beta$的所有线性无偏估计类当中，OLS估计的方差最小。即假设另有无偏估计$\\tilde{\\beta}_{j}$，若它可以表示为$y_i$的线性组合，则必有\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)<\\operatorname{Var}\\left(\\tilde{\\beta}_{j}\\right)\n",
    "$$\n",
    "值得注意的是，OLS只是在线性无偏估计中的方差最小，如果我们不追求估计的无偏性而只追求估计的稳定性（小方差），可以采用岭估计等有偏估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS系数估计的抽样分布-t分布**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1-MLR.5下，OLS估计是最优的线性无偏估计，这充分彰显了它的优越性。我们注意到，还有一个MLR.6正态性假设的没被用上，那么这个假设的作用是什么呢？——它确定了估计系数$\\hat{\\beta}_{j}$服从的分布，这为回归分析中最重要的一项功能——模型的假设检验打下了坚实基础。\n",
    "\n",
    "**· 定理4.** 在CLM假设**MLR.1-MLR.6**下，$\\hat{\\beta}_{j}$服从正态分布\n",
    "$$\n",
    "\\hat{\\beta}_{j} \\sim N\\left(\\beta_{j}, \\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)\\right)\n",
    "$$\n",
    "聪明的小伙伴们可能马上意识到，这样子不就有\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{s d\\left(\\hat{\\beta}_{j}\\right)} \\sim N(0,1)\n",
    "$$\n",
    "我们就可以使用正态分布进行假设检验了吗？非也！因为这里有一个致命的问题：标准差$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$在实际问题当中是无法求解的，我们也就无法通过构造一个含有$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$的检验统计量进行假设检验了，因为它无法被计算出来。不过这难不到统计学家们，因为标准误$\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)$是可以被计算出来的，并且有\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)} \\sim t_{n-k-1}\n",
    "$$\n",
    "也就是说，统计量$\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}$是服从t分布的，我们就可以使用t分布进行模型的假设检验了。注意：这些结论都建立在MLR.6正态性假设成立的基础上！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 回归分析的重要任务——推断/假设检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一个例子引出回归分析中推断任务的意义。\n",
    "\n",
    "**· Example3.** 依旧以上面的大学成绩例子为例。我们有一个直觉：成绩较好的学生似乎更倾向于不旷课，因此我们想知道旷课究竟会不会影响考试成绩。由于考试成绩取决于很多因素，我们要评价旷课(skipped)对成绩的影响，就需要建立一个多元线性回归模型，以控制其他因素的影响。通过OLS估计，方程结果如下\n",
    "$$\n",
    "\\mathrm{colGPA}=1.390+0.412\\mathrm{hsGPA}+0.015\\mathrm{ACT}-0.083\\mathrm{skipped}+u\n",
    "$$\n",
    "接下来的问题是，旷课skipped这一因素怎样子才能算是影响考试成绩呢？显然，如果它的系数非常接近0，那么它对成绩的影响是不明显的，换言之，是**不显著的**。事实上，我们后续在回归分析中所提及的“系数显著性”，本质上都是“**系数不为0的显著性**”。\n",
    "\n",
    "完成了旷课对成绩“显著影响”的解读，我们还需要解答最后一个问题：一个系数怎样子才算是接近0呢？skipped的系数是-0.083，它算是接近于0吗？显然我们无法回答这个问题，这个时候我们就需要借助概率论与数理统计中假设检验的知识来回答这一问题了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 t检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t检验是回归分析中单个线性假设检验问题的常用检验方法。单个线性假设检验问题可分为如下：\n",
    "\n",
    "· **单参数检验问题**：$H_{0}: \\beta_{j}=\\beta_{j 0} \\leftrightarrow H_{1}: \\beta_{j} \\neq \\beta_{j 0}$（$\\beta_{j 0}$为任意常数）\n",
    "\n",
    "这类问题的典型问题就是系数的显著性检验$H_{0}: \\beta_{j}=0 \\leftrightarrow H_{1}: \\beta_{j} \\neq 0$\n",
    "\n",
    "· **参数线性组合检验问题**：$H_0:f\\left( \\beta \\right) =\\beta _0\\leftrightarrow H_1:f\\left( \\beta \\right) \\ne \\beta _0$（$\\beta_{0}$为任意常数）\n",
    "\n",
    "这类问题的典型问题就是系数间的相等性检验$H_0:\\beta _i=\\beta _j\\leftrightarrow H_1:\\beta _i\\ne \\beta _j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 t检验的思想-从单参数检验说起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关假设检验严格的理论推导，大家可参考概率论与数理统计中的假设检验部分，在此我们只对其思想进行简单的介绍。\n",
    "\n",
    "正如上面介绍的，回归系数的显著性检验就是“系数是否为0”的检验\n",
    "$$\n",
    "H_{0}: \\beta_{j}=0 \\leftrightarrow H_{1}: \\beta_{j} \\neq 0\n",
    "$$\n",
    "如何根据样本数据对这一问题作出结论呢？我们的想法是，既然$\\hat{\\beta_{j}}$是$\\beta_{j}$的无偏估计，那么如果原假设成立，即真的有$\\beta_{j}=0$，那么$\\hat{\\beta_{j}}$有很大的可能性位于0附近；相反，如果实际样本计算出的$\\hat{\\beta_{j}}$远离0，那么这个假设**有很大可能不成立**。为了有一个确定的答案，我们设立一个临界值$C$，若$\\left|\\hat{\\beta}_{j}-0\\right|>C$，我们就拒绝假设$H_0$\n",
    "\n",
    "**· 临界值与置信水平**\n",
    "\n",
    "接下来的问题是，如何确定$C$呢？用概率。\n",
    "\n",
    "由于抽样的随机性，我们根据$\\hat{\\beta_{j}}$判断$\\beta_{j}$的命题，不论拒绝与否，都**有概率**会犯以下两类错误的其中之一：\n",
    "\n",
    "· 第一类错误，即原假设成立但是我们拒绝了它。犯第一类错误的概率称为拒真概率。\n",
    "\n",
    "· 第二类错误，即原假设不成立但是我们没有拒绝它。\n",
    "\n",
    "我们定夺临界值的时候，要保证发生第一类错误的概率需要在一个给定的、较小的水平$\\alpha$，这个$\\alpha$也被称为置信水平。如此以来，我们考虑临界值$C$的判准是，原假设$H_{0}$成立但是$\\left|\\hat{\\beta}_{j}-\\hat{\\beta}_{j0}\\right|>C$（因而拒绝原假设$H_{0}$）的概率应当恰好为我们人为给定的$\\alpha$，即\n",
    "$$\n",
    "P_{H_0\\,\\,is\\,\\,true}\\left( \\left| \\hat{\\beta}_j-\\beta _{j0} \\right|>C \\right) =P\\left( \\left| \\hat{\\beta}_j-0 \\right|>C \\right) =\\alpha \n",
    "$$\n",
    "\n",
    "**· 用t分布处理概率**\n",
    "\n",
    "现在我们就要开始处理$P\\left(\\left|\\hat{\\beta}_{j}-0\\right|>C\\right)$了。\n",
    "\n",
    "在前面的OLS估计的正态分布性质中我们得知$\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)} \\sim t_{n-k-1}$，而在原假设$H_{0}$成立时$\\beta_j=0$，因此$\\frac{\\hat{\\beta}_j}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}\\sim t_{n-k-1}$，我们便可以使用t分布处理以上概率\n",
    "$$\n",
    "P\\left(\\left|\\hat{\\beta}_{j}\\right|>C\\right)=P\\left(\\frac{\\left|\\hat{\\beta}_{j}\\right|}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}>\\frac{C}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}\\right)=\\alpha\n",
    "$$\n",
    "由于$\\frac{\\hat{\\beta}_j}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}$服从自由度为$n-k-1$的t分布，因此要让概率为$\\alpha$，$\\frac{C}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}$应等于$1-\\frac{\\alpha}{2}$分位点，记为$t_{n-k-1}\\left( 1-\\frac{\\alpha}{2} \\right) $\n",
    "\n",
    "于是，$C=t_{n-k-1}(1-\\alpha /2)\\mathrm{se}\\left( \\hat{\\beta}_j \\right) $，如果我们计算出来的$\\hat{\\beta_j}$有：$\\left| \\hat{\\beta}_j \\right|>t_{n-k-1}(1-\\alpha /2)\\mathrm{se}\\left( \\hat{\\beta}_j \\right) $，那么我们便可以拒绝原假设，这个系数是显著的！\n",
    "\n",
    "当然，在python实现的时候，我们不会直接比较临界值$C$与$|\\hat{\\beta}_j|$(因为计算$C$很麻烦)，而是先计算$\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}$，再与python输出的t分布对应的双侧分位点值$\\pm {t_{n-k-1}\\left( 1-\\frac{\\alpha}{2} \\right)} $进行比较。\n",
    "\n",
    "接下来，我们先进行手动假设检验，检验的问题为：\n",
    "$$\n",
    "H_{0}: \\beta_{3}=0 \\leftrightarrow H_{1}: \\beta_{3} \\neq 0\n",
    "$$\n",
    "置信水平为0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "双侧分位点为：(-1.9774312122928936, 1.9774312122928936)\n",
      "t值为：-3.1968396347468304\n",
      "t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数不为0.\n"
     ]
    }
   ],
   "source": [
    "# 手动进行假设检验\n",
    "gpa_lm3=sm.formula.ols('colGPA~hsGPA+ACT+skipped',data=gpa1).fit()\n",
    "\n",
    "## 计算t值\n",
    "skipped=gpa_lm3.params[3]\n",
    "se_skipped=gpa_lm3.bse[3]\n",
    "tvalue=skipped/se_skipped\n",
    "\n",
    "## 计算分位点\n",
    "from scipy.stats import t\n",
    "'''\n",
    "ppf:单侧左分位点\n",
    "isf:单侧右分位点\n",
    "interval:双侧分位点\n",
    "'''\n",
    "T_int=t.interval(0.95,gpa_lm3.df_resid) # 对于双侧检验（双侧分位点），分位点参数应该输入1-a，这里是1-0.05=0.95\n",
    "print('双侧分位点为：{}'.format(T_int))\n",
    "print('t值为：{}'.format(tvalue))\n",
    "print('t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数不为0.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们检验的问题还可以变为\n",
    "$$\n",
    "H_{0}: \\beta_{3}=-0.1 \\leftrightarrow H_{1}: \\beta_{3} \\neq -0.1\n",
    "$$\n",
    "我们只需要变更t值而不需要变更t分位点值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t值为：0.6495314591856163\n",
      "此时t值小于右侧分位点但大于左侧分位点，位于接受域，不能拒绝原假设，即skipped系数可为-0.1\n"
     ]
    }
   ],
   "source": [
    "tvalue=(skipped+0.1)/se_skipped\n",
    "print('t值为：{}'.format(tvalue))\n",
    "print('此时t值小于右侧分位点但大于左侧分位点，位于接受域，不能拒绝原假设，即skipped系数可为-0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 单边检验——换汤不换药**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们介绍的是t检验的双边检验，它的假设是这样的\n",
    "$$H_{0}: \\beta_{j}=\\beta_{j 0} \\leftrightarrow H_{1}: \\beta_{j} \\neq \\beta_{j 0}$$\n",
    "双边检验回答的问题是：实际参数是否“靠近”我们假设的值。而有时候我们回归分析中可能还会有这样的问题：某某自变量对因变量是否存在正效应影响呢？这个问题其实等价于下面的假设\n",
    "$$\n",
    "H_0:\\beta _j=\\beta _{j0}\\leftrightarrow H_1:\\beta _j>\\beta _{j0}\\,\\,\\left( \\beta _{j0}=0 \\right) \n",
    "$$\n",
    "单边检验的分析思路和双边检验基本一样，只不过$P\\left(\\left|\\hat{\\beta}_{j}-0\\right|>C\\right)$要变为$P\\left( \\hat{\\beta}_j-0>C \\right) $，$\\frac{C}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}$也应从$1-\\frac{\\alpha}{2}$分位点变为$1-\\alpha $分位点(大家可以思考一下为什么)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们依旧进行手动检验，检验问题为\n",
    "$$\n",
    "H_0:\\beta _j=0\\leftrightarrow H_1:\\beta _j<0\n",
    "$$\n",
    "置信水平为0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "左侧分位点为：-1.6560520804924401\n",
      "t值为：-3.1968396347468304\n",
      "t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数小于0.\n"
     ]
    }
   ],
   "source": [
    "tvalue=skipped/se_skipped\n",
    "# 因为是小于，因此看左分位点\n",
    "T_right=t.ppf(0.05,gpa_lm3.df_resid) # 对于单侧检验，分位点参数应该输入a，这里是0.05\n",
    "print('左侧分位点为：{}'.format(T_right))\n",
    "print('t值为：{}'.format(tvalue))\n",
    "print('t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数小于0.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· p值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用临界值$C$与$\\left|\\hat{\\beta}_{j}-\\hat{\\beta}_{j0}\\right|$作比较有一个缺点，就是分位点值与置信水平$\\alpha$相关的。如果我们要在不同的置信水平下检验，就需要计算不同的分位点再比较，这样很繁琐。这个时候，我们可以使用p值。\n",
    "\n",
    "p值是在本次分析的样本观测值下，给出的能拒绝原假设的最小置信水平，它只与样本观测值和我们做的假设检验有关。p值越小越可以拒绝原假设，例如：如果p值为0.001，比0.01的置信水平还要小，我们认为在0.01的置信水平下我们也可以拒绝原假设；而如果p值为0.025，比0.01的置信水平要大，但小于0.05，则我们认为在0.05的置信水平下我们可以拒绝原假设，但在0.01置信水平下不可以拒绝。\n",
    "\n",
    "p值的形式与我们做的备择假设$H_1$有关：\n",
    "\n",
    "· 若$H_{1}: \\beta_{j} \\neq \\beta_{j 0}$，则：$pvalue=P\\left( \\left| t_{n-k-1} \\right|>\\left| \\frac{\\hat{\\beta}_j-\\beta _{j0}}{se\\left( \\hat{\\beta}_j \\right)} \\right| \\right) $\n",
    "\n",
    "· 若$H_{1}: \\beta_{j} > \\beta_{j 0}$，则：$pvalue=P\\left( t_{n-k-1}>\\frac{\\hat{\\beta}_j-\\beta _{j0}}{se\\left( \\hat{\\beta}_j \\right)} \\right) $\n",
    "\n",
    "· 若$H_{1}: \\beta_{j} < \\beta_{j 0}$，则：$pvalue=P\\left( t_{n-k-1}<\\frac{\\hat{\\beta}_j-\\beta _{j0}}{se\\left( \\hat{\\beta}_j \\right)} \\right) $\n",
    "\n",
    "可以看到，p值本质上是一种累积概率，且对于同一个$\\beta_{j 0}$而言，双边检验的p值为单边检验的两倍（在代码实现中我们可以看到这点）。我们先利用p值手动检验以下问题\n",
    "$$\n",
    "H_{0}: \\beta_{3}=0 \\leftrightarrow H_{1}: \\beta_{3} \\neq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "由于双边检验p值是对单边检验p值乘两倍得来的，我们要根据t值是否大于0来选择左/右尾累积概率，若小于0，则选择左尾；反之右尾。\n",
      "True\n",
      "p值为：0.002\n",
      "p值非常小，可见我们可以拒绝原假设\n"
     ]
    }
   ],
   "source": [
    "# 计算t值仍然是第一步\n",
    "tvalue=skipped/se_skipped\n",
    "print('由于双边检验p值是对单边检验p值乘两倍得来的，我们要根据t值是否大于0来选择左/右尾累积概率，若小于0，则选择左尾；反之右尾。')\n",
    "'''\n",
    "sf:右尾累积概率\n",
    "cdf:左尾累积概率\n",
    "'''\n",
    "print(tvalue<0)\n",
    "pvalue=t.cdf(tvalue,gpa_lm3.df_resid)*2 # 双边p值记得乘2\n",
    "print('p值为：{:.3f}'.format(pvalue)) # 保留三位小数\n",
    "print('p值非常小，可见我们可以拒绝原假设')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，summary中的p值，正是系数0值双边检验的p值，我们查看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 colGPA   R-squared:                       0.234\n",
      "Model:                            OLS   Adj. R-squared:                  0.217\n",
      "Method:                 Least Squares   F-statistic:                     13.92\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           5.65e-08\n",
      "Time:                        15:37:16   Log-Likelihood:                -41.501\n",
      "No. Observations:                 141   AIC:                             91.00\n",
      "Df Residuals:                     137   BIC:                             102.8\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.3896      0.332      4.191      0.000       0.734       2.045\n",
      "hsGPA          0.4118      0.094      4.396      0.000       0.227       0.597\n",
      "ACT            0.0147      0.011      1.393      0.166      -0.006       0.036\n",
      "skipped       -0.0831      0.026     -3.197      0.002      -0.135      -0.032\n",
      "==============================================================================\n",
      "Omnibus:                        1.917   Durbin-Watson:                   1.881\n",
      "Prob(Omnibus):                  0.383   Jarque-Bera (JB):                1.636\n",
      "Skew:                           0.125   Prob(JB):                        0.441\n",
      "Kurtosis:                       2.535   Cond. No.                         300.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(gpa_lm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，对于系数的非0值单边检验，我们也可以进行手动检验，考虑下面问题\n",
    "$$\n",
    "H_{0}: \\beta_{3}=-0.1 \\leftrightarrow H_{1}: \\beta_{3} > -0.1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p值为：0.259\n",
      "p值远大于0.1，可见我们不能拒绝原假设\n"
     ]
    }
   ],
   "source": [
    "# 还是先计算t值！\n",
    "tvalue=(skipped+0.1)/se_skipped\n",
    "pvalue=t.sf(tvalue,gpa_lm3.df_resid) # 由于备择假设是大于号，因此要用右尾累积概率，且不用乘2\n",
    "print('p值为：{:.3f}'.format(pvalue)) # 保留三位小数\n",
    "print('p值远大于0.1，可见我们不能拒绝原假设')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 参数线性组合的检验-巧用模型变式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的t检验里，我们进行的检验都是对单个参数的检验。事实上，如果我们要对多个参数之间的关系进行假设检验，也可以用t检验，这个时候，我们本质上是对参数的线性组合进行检验。我们依旧用一个例子引入该问题。\n",
    "\n",
    "**· Example4.** 我们现在探讨一个有关薪酬的问题，想看看哪些因素会影响我们的薪酬。经过一番思考，我们先将模型设置为\n",
    "\n",
    "$$\n",
    "\\log (\\text { wage })=\\beta_{0}+\\beta_{1} j c+\\beta_{2} u n i v+\\beta_{3} \\operatorname{exper}+u\n",
    "$$\n",
    "其中，jc表示为大专教育年限，univ为大学教育年限，exper为工作年限。我们想知道：大专学历的边际回报是否不如大学学历的边际回报，这等价于下面的假设检验\n",
    "$$\n",
    "H_{0}: \\beta_{1}=\\beta_{2} \\leftrightarrow H_{1}: \\beta_{1}<\\beta_{2}\n",
    "$$\n",
    "而这又可以变形为\n",
    "$$\n",
    "H_0:\\beta _1-\\beta _2=0\\leftrightarrow H_1:\\beta _1-\\beta _2<0\n",
    "$$\n",
    "我们依旧可以采用前面的思路，先构造t检验统计量\n",
    "$$\n",
    "t=\\frac{\\hat{\\beta}_1-\\hat{\\beta}_2}{se\\left( \\hat{\\beta}_1-\\hat{\\beta}_2 \\right)}\n",
    "$$\n",
    "再根据t分布求得p值即可。问题是$se\\left( \\hat{\\beta}_1-\\hat{\\beta}_2 \\right)$的求解不那么容易，需要使用协方差矩阵$Cov\\left( \\vec{\\hat{\\beta}} \\right) $内的方差与协方差。当线性组合变得复杂的时候，这样的任务将变得更加困难。于是我们另辟蹊径，用一种很巧妙的方法完成这类假设检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 巧变模型**\n",
    "\n",
    "既然假设检验的问题是\n",
    "$$\n",
    "H_{0}: \\beta_{1}=\\beta_{2} \\leftrightarrow H_{1}: \\beta_{1}<\\beta_{2}\n",
    "$$\n",
    "我们干脆令$\\theta_{1}=\\beta_{1}-\\beta_{2}$，于是$\\beta_{1}=\\theta_{1}+\\beta_{2}$，将其代入到原式中并将带有系数$\\theta _1$的一项提出来，得\n",
    "$$\n",
    "\\log (\\text { wage })=\\beta_{0}+\\theta_{1} j c+\\beta_{2}(j c+\\text { univ })+\\beta_{3} \\operatorname{exper}+u\n",
    "$$\n",
    "记$j c+u n i v= totcoll$，是两个变量之和，此时模型简化为\n",
    "$$\n",
    "\\log (\\text { wage })=\\beta_{0}+\\theta_{1} \\text { jc }+\\beta_{2} \\text { totcoll }+\\beta_{3} \\text { exper }+u\n",
    "$$\n",
    "原检验问题也变为了\n",
    "$$\n",
    "H_{0}: \\theta=0 \\leftrightarrow H_{1}: \\theta<0\n",
    "$$\n",
    "此时，问题有转化为了对**新模型**的单个参数的显著性检验问题。注意，这个新模型的意义仅仅只在于做假设检验，虽然两个模型实际上是等价的。（大家可以比较两者其他的输出）\n",
    "\n",
    "接下来进行python实操演练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>phsrank</th>\n",
       "      <th>BA</th>\n",
       "      <th>AA</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>id</th>\n",
       "      <th>exper</th>\n",
       "      <th>jc</th>\n",
       "      <th>univ</th>\n",
       "      <th>...</th>\n",
       "      <th>medcity</th>\n",
       "      <th>submed</th>\n",
       "      <th>lgcity</th>\n",
       "      <th>sublg</th>\n",
       "      <th>vlgcity</th>\n",
       "      <th>subvlg</th>\n",
       "      <th>ne</th>\n",
       "      <th>nc</th>\n",
       "      <th>south</th>\n",
       "      <th>totcoll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.033333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>119.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  phsrank  BA  AA  black  hispanic     id  exper        jc      univ  \\\n",
       "0       1       65   0   0      0         0   19.0    161  0.000000  0.000000   \n",
       "1       1       97   0   0      0         0   93.0    119  0.000000  7.033333   \n",
       "2       1       44   0   0      0         0   96.0     81  0.000000  0.000000   \n",
       "3       1       34   0   0      0         1  119.0     39  0.266667  0.000000   \n",
       "4       1       80   0   0      0         0  132.0    141  0.000000  0.000000   \n",
       "\n",
       "   ...  medcity  submed  lgcity  sublg  vlgcity  subvlg  ne  nc  south  \\\n",
       "0  ...        0       0       0      1        0       0   1   0      0   \n",
       "1  ...        0       0       0      0        0       0   0   1      0   \n",
       "2  ...        0       0       0      1        0       0   1   0      0   \n",
       "3  ...        0       0       0      0        0       0   0   0      0   \n",
       "4  ...        0       0       0      0        0       0   0   0      1   \n",
       "\n",
       "    totcoll  \n",
       "0  0.000000  \n",
       "1  7.033333  \n",
       "2  0.000000  \n",
       "3  0.266667  \n",
       "4  0.000000  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wage1=pd.read_stata('./data/twoyear.dta')\n",
    "wage1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  lwage   R-squared:                       0.222\n",
      "Model:                            OLS   Adj. R-squared:                  0.222\n",
      "Method:                 Least Squares   F-statistic:                     644.5\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):               0.00\n",
      "Time:                        15:37:17   Log-Likelihood:                -3888.7\n",
      "No. Observations:                6763   AIC:                             7785.\n",
      "Df Residuals:                    6759   BIC:                             7813.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        1.4723      0.021     69.910      0.000       1.431       1.514\n",
      "jc              -0.0102      0.007     -1.468      0.142      -0.024       0.003\n",
      "I(jc + univ)     0.0769      0.002     33.298      0.000       0.072       0.081\n",
      "exper            0.0049      0.000     31.397      0.000       0.005       0.005\n",
      "==============================================================================\n",
      "Omnibus:                       81.514   Durbin-Watson:                   1.968\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              142.465\n",
      "Skew:                          -0.036   Prob(JB):                     1.16e-31\n",
      "Kurtosis:                       3.707   Cond. No.                         511.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "sm.formula.ols与sm.ols不同，其最大的特点是可以指定模型的形式，这非常有利于我们自主的构建模型，此后我们将统一使用该指令。\n",
    "值得注意的是，sm.formula.ols默认带截距项\n",
    "'''\n",
    "wage1_lm=sm.formula.ols('lwage~jc+I(jc+univ)+exper',data=wage1).fit()\n",
    "# 注意，如果我们要将jc与univ的和当做一个新变量的话，需要使用I()\n",
    "print(wage1_lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里告诉大家巧用summary中的双侧p值进行单侧0值检验的小技巧。在summary中，jc的t值小于0，说明它后面的双侧p值是使用左侧累积概率乘两倍得来的，而在本例中我们的备择假设是小于0，p值也应当是左侧累积概率，因此我们只需将报告表中的p值除以2即可。而如果t值小于0，但是备择假设却大于0，那么无需思考，p值一定大于0.5，我们肯定不能拒绝原假设。\n",
    "\n",
    "在本例中，由于我们做的是小于0的假设，jc的t值也小于0，因此这一假设检验的p值应当为0.142/2=0.071。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 F检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F检验是回归分析中**多个**线性假设检验问题的常用检验方法。多个线性假设检验问题可分为如下：\n",
    "\n",
    "· **多参数联合显著性检验问题**：$H_{0}: \\beta _i=\\cdots =\\beta _j=0 \\leftrightarrow H_{1}: $ $H_{0}$不成立\n",
    "\n",
    "· **一般多参数检验问题**：$H_0:\\beta _n=\\beta _{n0}\\,\\,, \\beta _i=\\cdots =\\beta _j=0\\leftrightarrow H_1: $ $H_{0}$不成立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 F检验的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际问题的假设检验中，我们除了对某个参数单独进行检验外，还需要对多个参数同时进行检验，我们用一个例子来说明。\n",
    "\n",
    "**· Example5.** 考虑美国棒球职业大联盟的运动员薪水问题，假设模型为\n",
    "$$\n",
    "\\log (\\text { salary })=\\beta_{0}+\\beta_{1} \\text { years }+\\beta_{2} \\text { gamesyr }+\\beta_{3} \\text { bavg }+\\beta_{4} \\text { hrunsyr }+\\beta_{5} \\text { rbisyr }+u\n",
    "$$\n",
    "其中，salary是队员薪水，years为加入联盟的年限，gamesyr为每年参加比赛的次数，bavg是击球率，hrunsyr为本垒打次数，rbisyr表示击球跑垒得分。后面三个指标是运动员的球场表现正向指标（指标越高，代表表现越好），而前面两个指标则为运动员的球场资历指标。\n",
    "\n",
    "我们想弄明白一个问题：运动员的表现正向指标是否对薪水有显著影响。如何理解这一问题？如果这三个指标中至少有一个指标系数显著不为0，我们便可以认为表现正向指标对薪资有显著影响。于是原假设可以设置为\n",
    "$$\n",
    "H_{0}: \\beta_{3}=0, \\beta_{4}=0, \\beta_{5}=0\n",
    "$$\n",
    "对立假设则为：原假设不成立。\n",
    "\n",
    "注意，三个参数做联合显著性检验**完全不等价于**三个参数分开做显著性t检验！如果我们是出于联合检验的目的但是却做了分开检验，将大大增加拒真概率。由于无法分开始用t检验进行联合检验，我们需要一种新的检验方法——F检验。\n",
    "\n",
    "**· F统计量的定义——约束模型与无约束模型的比较**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在回归分析中**，F检验更像是在比较两种模型的差异程度。\n",
    "\n",
    "首先，我们称原模型为无约束模型(unrestricted model)：\n",
    "$$\n",
    "\\log (\\text { salary })=\\beta_{0}+\\beta_{1} \\text { years }+\\beta_{2} \\text { gamesyr }+\\beta_{3} \\text { bavg }+\\beta_{4} \\text { hrunsyr }+\\beta_{5} \\text { rbisyr }+u\n",
    "$$\n",
    "然后将原假设$H_0$成立下的条件代入无约束模型，得到的模型称为有约束模型(restricted model)：\n",
    "$$\n",
    "\\log (\\text { salary })=\\beta_{0}+\\beta_{1} \\text { years }+\\beta_{2} \\text { gamesyr }+u\n",
    "$$\n",
    "无约束模型相较于有约束模型多了三个参数与变量。一般而言，模型变量越多，对训练集数据的变异解释程度会越高，拟合优度会越好，进而残差平方和会减小。**如果两个模型残差平方和的差异足够大，说明原假设约束的加入是模型产生了显著性的变化，这意味着原假设是显著的！**\n",
    "\n",
    "基于这种思考，统计学家们定义了回归分析中的F检验统计量\n",
    "$$\n",
    "F=\\frac{\\left( RSS_r-RSS_{ur} \\right) /q}{RSS_{ur}/(n-k-1)}\\sim F_{q,n-k-1}\n",
    "$$\n",
    "它服从自由度为$q$与$n-k-1$的F分布，其中$q$为有效约束个数，$n-k-1$为无约束模型自由度。\n",
    "\n",
    "F检验拒绝原假设的判别规则非常简单，即\n",
    "$$\n",
    "F>F_{q,n-k-1}\\left( 1-\\alpha \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们进行F联合检验的python实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>teamsal</th>\n",
       "      <th>nl</th>\n",
       "      <th>years</th>\n",
       "      <th>games</th>\n",
       "      <th>atbats</th>\n",
       "      <th>runs</th>\n",
       "      <th>hits</th>\n",
       "      <th>doubles</th>\n",
       "      <th>triples</th>\n",
       "      <th>...</th>\n",
       "      <th>runsyr</th>\n",
       "      <th>percwhte</th>\n",
       "      <th>percblck</th>\n",
       "      <th>perchisp</th>\n",
       "      <th>blckpb</th>\n",
       "      <th>hispph</th>\n",
       "      <th>whtepw</th>\n",
       "      <th>blckph</th>\n",
       "      <th>hisppb</th>\n",
       "      <th>lsalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6329213.0</td>\n",
       "      <td>38407380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>6705.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>89.666656</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>18.844231</td>\n",
       "      <td>10.8778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.66069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3375000.0</td>\n",
       "      <td>38407380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>863.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.875000</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>18.844231</td>\n",
       "      <td>10.8778</td>\n",
       "      <td>18.844231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.8778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.03191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3100000.0</td>\n",
       "      <td>38407380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>2807.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>840.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>18.844231</td>\n",
       "      <td>10.8778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.94691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2900000.0</td>\n",
       "      <td>38407380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>3337.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.625000</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>18.844231</td>\n",
       "      <td>10.8778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.88022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1650000.0</td>\n",
       "      <td>38407380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>3603.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>928.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.416672</td>\n",
       "      <td>70.277969</td>\n",
       "      <td>18.844231</td>\n",
       "      <td>10.8778</td>\n",
       "      <td>18.844231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.8778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.31629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      salary     teamsal   nl  years   games  atbats    runs    hits  doubles  \\\n",
       "0  6329213.0  38407380.0  1.0   12.0  1705.0  6705.0  1076.0  1939.0    320.0   \n",
       "1  3375000.0  38407380.0  1.0    8.0   918.0  3333.0   407.0   863.0    156.0   \n",
       "2  3100000.0  38407380.0  1.0    5.0   751.0  2807.0   370.0   840.0    148.0   \n",
       "3  2900000.0  38407380.0  1.0    8.0  1056.0  3337.0   405.0   816.0    143.0   \n",
       "4  1650000.0  38407380.0  1.0   12.0  1196.0  3603.0   437.0   928.0     19.0   \n",
       "\n",
       "   triples  ...     runsyr   percwhte   percblck  perchisp     blckpb  hispph  \\\n",
       "0     67.0  ...  89.666656  70.277969  18.844231   10.8778   0.000000     0.0   \n",
       "1     38.0  ...  50.875000  70.277969  18.844231   10.8778  18.844231     0.0   \n",
       "2     18.0  ...  74.000000  70.277969  18.844231   10.8778   0.000000     0.0   \n",
       "3     18.0  ...  50.625000  70.277969  18.844231   10.8778   0.000000     0.0   \n",
       "4     16.0  ...  36.416672  70.277969  18.844231   10.8778  18.844231     0.0   \n",
       "\n",
       "      whtepw   blckph  hisppb   lsalary  \n",
       "0  70.277969   0.0000     0.0  15.66069  \n",
       "1   0.000000  10.8778     0.0  15.03191  \n",
       "2  70.277969   0.0000     0.0  14.94691  \n",
       "3  70.277969   0.0000     0.0  14.88022  \n",
       "4   0.000000  10.8778     0.0  14.31629  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入数据集\n",
    "mlb1=pd.read_stata('./data/mlb1.dta')\n",
    "mlb1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先进行手动假设检验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F值为：9.550271724244364\n",
      "F分位点为：2.6306414891156504\n",
      "F值位列分位点右侧，说明位于拒绝域当中，可以在显著性水平0.05下拒绝原假设\n",
      "p值为：0.000004\n"
     ]
    }
   ],
   "source": [
    "# 无约束模型\n",
    "mlb_ur=sm.formula.ols('lsalary~years+gamesyr+bavg+hrunsyr+rbisyr',data=mlb1).fit()\n",
    "# 有约束模型\n",
    "mlb_r=sm.formula.ols('lsalary~years+gamesyr',data=mlb1).fit()\n",
    "\n",
    "# 计算两个模型的RSS\n",
    "RSS_mlb_ur=np.sum(np.power(mlb_ur.resid,2))\n",
    "RSS_mlb_r=np.sum(np.power(mlb_r.resid,2))\n",
    "\n",
    "# 计算F统计量\n",
    "Fvalue=((RSS_mlb_r-RSS_mlb_ur)/3)/(RSS_mlb_ur/(mlb_ur.df_resid))\n",
    "print('F值为：{}'.format(Fvalue))\n",
    "\n",
    "# 计算F分布分位点\n",
    "from scipy.stats import f\n",
    "# 由于F检验只有大于号的假设，因此只会使用单侧右分位点\n",
    "F_isf=f.isf(0.05,3,mlb_ur.df_resid) # 注意自由度的顺序不能颠倒,这里显著性水平为0.05\n",
    "print('F分位点为：{}'.format(F_isf))\n",
    "print('F值位列分位点右侧，说明位于拒绝域当中，可以在显著性水平0.05下拒绝原假设')\n",
    "\n",
    "# 计算p值\n",
    "# 由于F检验只有大于号的假设，因此只会使用单侧右分位点\n",
    "pvalue=f.sf(Fvalue,3,mlb_ur.df_resid)\n",
    "print('p值为：{:.6f}'.format(pvalue))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以直接使用anova_lm函数，它会直接输出F值与p值，非常方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_resid</th>\n",
       "      <th>ssr</th>\n",
       "      <th>df_diff</th>\n",
       "      <th>ss_diff</th>\n",
       "      <th>F</th>\n",
       "      <th>Pr(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350.0</td>\n",
       "      <td>198.311502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347.0</td>\n",
       "      <td>183.186322</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.12518</td>\n",
       "      <td>9.550272</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   df_resid         ssr  df_diff   ss_diff         F    Pr(>F)\n",
       "0     350.0  198.311502      0.0       NaN       NaN       NaN\n",
       "1     347.0  183.186322      3.0  15.12518  9.550272  0.000004"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.anova import anova_lm\n",
    "anova_lm(mlb_r,mlb_ur) # 注意，是有约束在前，无约束在后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 一般多参数检验问题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了全零假设，原假设还可以更一般地设置为\n",
    "$$\n",
    "H_0:\\beta _n=\\beta _{n0}\\,\\,, \\beta _i=\\cdots =\\beta _j=0\n",
    "$$\n",
    "即，部分假设可以设置为非0参数。对于这种检验问题，我们的有约束模型需要将假设中非0参数的变量移至因变量一侧。例如，若假设为\n",
    "$$\n",
    "H_{0}: \\beta_{3}=1, \\beta_{4}=0, \\beta_{5}=0\n",
    "$$\n",
    "则有约束模型为\n",
    "$$\n",
    "\\log (\\text { salary })-\\text{bavg}=\\beta_{0}+\\beta_{1} \\text { years }+\\beta_{2} \\text { gamesyr }+u\n",
    "$$\n",
    "这意味着有约束模型的因变量发生了改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>assess</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>lotsize</th>\n",
       "      <th>sqrft</th>\n",
       "      <th>colonial</th>\n",
       "      <th>lprice</th>\n",
       "      <th>lassess</th>\n",
       "      <th>llotsize</th>\n",
       "      <th>lsqrft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>349.100006</td>\n",
       "      <td>4</td>\n",
       "      <td>6126.0</td>\n",
       "      <td>2438</td>\n",
       "      <td>1</td>\n",
       "      <td>5.703783</td>\n",
       "      <td>5.855359</td>\n",
       "      <td>8.720297</td>\n",
       "      <td>7.798934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>370.0</td>\n",
       "      <td>351.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>9903.0</td>\n",
       "      <td>2076</td>\n",
       "      <td>1</td>\n",
       "      <td>5.913503</td>\n",
       "      <td>5.862210</td>\n",
       "      <td>9.200593</td>\n",
       "      <td>7.638198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191.0</td>\n",
       "      <td>217.699997</td>\n",
       "      <td>3</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>1374</td>\n",
       "      <td>0</td>\n",
       "      <td>5.252274</td>\n",
       "      <td>5.383118</td>\n",
       "      <td>8.556414</td>\n",
       "      <td>7.225482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195.0</td>\n",
       "      <td>231.800003</td>\n",
       "      <td>3</td>\n",
       "      <td>4600.0</td>\n",
       "      <td>1448</td>\n",
       "      <td>1</td>\n",
       "      <td>5.273000</td>\n",
       "      <td>5.445875</td>\n",
       "      <td>8.433811</td>\n",
       "      <td>7.277938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373.0</td>\n",
       "      <td>319.100006</td>\n",
       "      <td>4</td>\n",
       "      <td>6095.0</td>\n",
       "      <td>2514</td>\n",
       "      <td>1</td>\n",
       "      <td>5.921578</td>\n",
       "      <td>5.765504</td>\n",
       "      <td>8.715224</td>\n",
       "      <td>7.829630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price      assess  bdrms  lotsize  sqrft  colonial    lprice   lassess  \\\n",
       "0  300.0  349.100006      4   6126.0   2438         1  5.703783  5.855359   \n",
       "1  370.0  351.500000      3   9903.0   2076         1  5.913503  5.862210   \n",
       "2  191.0  217.699997      3   5200.0   1374         0  5.252274  5.383118   \n",
       "3  195.0  231.800003      3   4600.0   1448         1  5.273000  5.445875   \n",
       "4  373.0  319.100006      4   6095.0   2514         1  5.921578  5.765504   \n",
       "\n",
       "   llotsize    lsqrft  \n",
       "0  8.720297  7.798934  \n",
       "1  9.200593  7.638198  \n",
       "2  8.556414  7.225482  \n",
       "3  8.433811  7.277938  \n",
       "4  8.715224  7.829630  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprice1=pd.read_stata('./data/hprice1.dta')\n",
    "hprice1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p值远大于0.1，不能拒绝原假设\n"
     ]
    }
   ],
   "source": [
    "hprice_ur=sm.formula.ols('lprice~lassess+llotsize+lsqrft+bdrms',data=hprice1).fit()\n",
    "hprice_r=sm.formula.ols('I(lprice-lassess)~1',data=hprice1).fit() \n",
    "# 注意，将lassess移至因变量后，它们的差应视作一个整体回归元，因此需要添加I()\n",
    "anova_lm(hprice_r,hprice_ur) # 注意，是有约束在前，无约束在后\n",
    "print('p值远大于0.1，不能拒绝原假设')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 更广义的“线性”回归——多种形式自变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面的章节，我们学习了最经典、最简单的多元线性回归模型\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "为了大家在学习之初不被混淆，我们其实默认了自变量都是一次项的、定量的变量。实际上，自变量不仅可以是一次的连续变量，还可以是一种**定性变量**，也可以是某个**变量的函数**，如二次项$X^2$、对数项$log(X)$。这是因为，所谓的线性回归模型，线性关系并不是指代被解释变量$y$与解释变量$X$之间的关系，而是指回归函数相对于**回归系数**是线性的。\n",
    "\n",
    "在这一章节，我们将重点学习带有定性变量的回归，并简单介绍常用的带有变量函数的回归（如带有对数项）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 带有定性变量的回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们所考虑的模型里的变量都是定量变量，如：工资，产品销售量等，其取值有大小的区分。而在实际问题中，还有一些诸如性别、种族、季节、婚姻状态等定性变量，也称为类别变量。我们只讨论自变量带有定性变量的情况，不讨论因变量是定性变量的情况，因为此时问题将变为分类问题而非回归问题。\n",
    "\n",
    "我们先讨论最简单的二分类变量，再讨论多分类变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 二分类变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 定性变量定量化——虚拟变量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定性变量的取值一般都不是数字，如性别变量的取值为男/女、婚姻状态的取值为是/否，计算机肯定是无法识别这些非数字的，这个时候我们就要将它们定量化了！\n",
    "\n",
    "最方便也是最高效的定量化就是用0-1变量定义二分类变量。一般而言，0表示“否”，1表示“是”，对于性别$sex$这个变量，我们可以将其转化一个“女士变量”$female$，当$female=1$时表示样本为女士，当$female=0$时表示样本为男士。当然，我们定义$sex=0$为男士，$sex=1$为女士也是完全没问题的。\n",
    "\n",
    "这种替代定性变量性别的$female$变量被称为虚拟变量/哑变量(Dummy Variable)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 定性变量系数的解读**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明确了0-1虚拟变量的含义，我们便可以研究虚拟变量在回归中的含义了。我们依旧用一个例子代入讨论，让大家可以更直观地理解。\n",
    "\n",
    "**· Example6.** 我们想知道男女在职场上是否存在薪资不平等的情况，就要在控制其他变量的前提下，观察性别差异给薪资带来的影响。我们考虑下述模型\n",
    "$$\n",
    "\\text { wage }=\\beta_{0}+\\delta_{0} \\text { female }+\\beta_{1} e d u c+u\n",
    "$$\n",
    "如何理解定性变量的回归系数$\\delta_{0}$呢，我们知道当$female=1$时表示样本为女性，当$female=0$时表示样本为男性，于是男性的回归函数为\n",
    "$$\n",
    "E(\\text { wage } \\mid \\text { male }, e d u c)=\\beta_{0}+\\beta_{1} \\text { educ }\n",
    "$$\n",
    "女性的回归函数为\n",
    "$$\n",
    "E\\left(\\text { wage }[\\text { female }, \\text { educ })=\\beta_{0}+\\delta_{0}+\\beta_{1} e d u c\\right.\n",
    "$$\n",
    "它们直观的区别如下图所示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/定性.png'>\n",
    "两者相差的数值处处为常数，它们的差本质上源于截距的不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，$\\delta_{0}$就是\n",
    "$$\n",
    "\\delta_{0}=E(\\text { wage|female }, e d u c)-E(\\text { wage|male, educ })\n",
    "$$\n",
    "因此，$\\delta_{0}$表示的是在同等受教育水平下，女性与男性的工资差异。而如果$\\delta_{0}<0$，就说明同等教育水平下，女性工资比男性工资低，职场可能存在性别歧视。我们用python就该问题做一次假设检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage</th>\n",
       "      <th>educ</th>\n",
       "      <th>exper</th>\n",
       "      <th>tenure</th>\n",
       "      <th>nonwhite</th>\n",
       "      <th>female</th>\n",
       "      <th>married</th>\n",
       "      <th>numdep</th>\n",
       "      <th>smsa</th>\n",
       "      <th>northcen</th>\n",
       "      <th>...</th>\n",
       "      <th>trcommpu</th>\n",
       "      <th>trade</th>\n",
       "      <th>services</th>\n",
       "      <th>profserv</th>\n",
       "      <th>profocc</th>\n",
       "      <th>clerocc</th>\n",
       "      <th>servocc</th>\n",
       "      <th>lwage</th>\n",
       "      <th>expersq</th>\n",
       "      <th>tenursq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.131402</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.24</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.175573</td>\n",
       "      <td>484.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.30</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.667707</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage  educ  exper  tenure  nonwhite  female  married  numdep  smsa  \\\n",
       "0  3.10  11.0    2.0     0.0       0.0     1.0      0.0     2.0   1.0   \n",
       "1  3.24  12.0   22.0     2.0       0.0     1.0      1.0     3.0   1.0   \n",
       "2  3.00  11.0    2.0     0.0       0.0     0.0      0.0     2.0   0.0   \n",
       "3  6.00   8.0   44.0    28.0       0.0     0.0      1.0     0.0   1.0   \n",
       "4  5.30  12.0    7.0     2.0       0.0     0.0      1.0     1.0   0.0   \n",
       "\n",
       "   northcen  ...  trcommpu  trade  services  profserv  profocc  clerocc  \\\n",
       "0       0.0  ...       0.0    0.0       0.0       0.0      0.0      0.0   \n",
       "1       0.0  ...       0.0    0.0       1.0       0.0      0.0      0.0   \n",
       "2       0.0  ...       0.0    1.0       0.0       0.0      0.0      0.0   \n",
       "3       0.0  ...       0.0    0.0       0.0       0.0      0.0      1.0   \n",
       "4       0.0  ...       0.0    0.0       0.0       0.0      0.0      0.0   \n",
       "\n",
       "   servocc     lwage  expersq  tenursq  \n",
       "0      0.0  1.131402      4.0      0.0  \n",
       "1      1.0  1.175573    484.0      4.0  \n",
       "2      0.0  1.098612      4.0      0.0  \n",
       "3      0.0  1.791759   1936.0    784.0  \n",
       "4      0.0  1.667707     49.0      4.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wage1=pd.read_stata('./data/wage1.dta')\n",
    "wage1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   wage   R-squared:                       0.364\n",
      "Model:                            OLS   Adj. R-squared:                  0.359\n",
      "Method:                 Least Squares   F-statistic:                     74.40\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           7.30e-50\n",
      "Time:                        15:37:19   Log-Likelihood:                -1314.2\n",
      "No. Observations:                 526   AIC:                             2638.\n",
      "Df Residuals:                     521   BIC:                             2660.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -1.5679      0.725     -2.164      0.031      -2.991      -0.145\n",
      "female        -1.8109      0.265     -6.838      0.000      -2.331      -1.291\n",
      "educ           0.5715      0.049     11.584      0.000       0.475       0.668\n",
      "exper          0.0254      0.012      2.195      0.029       0.003       0.048\n",
      "tenure         0.1410      0.021      6.663      0.000       0.099       0.183\n",
      "==============================================================================\n",
      "Omnibus:                      185.864   Durbin-Watson:                   1.794\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              715.580\n",
      "Skew:                           1.589   Prob(JB):                    4.11e-156\n",
      "Kurtosis:                       7.749   Cond. No.                         141.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "wage1_lm=sm.formula.ols('wage~female+educ+exper+tenure',data=wage1).fit()\n",
    "print(wage1_lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们做的是0值单边假设，因此可以通过summary汇总表中的p值判断。female的t值小于0，因此其p值采用的是左尾累积概率；而我们的假设是小于0假设，也采用的是左尾累积概率，因此我们只需要将报告表中的p值除以2即可。显然，female的p值在保留三位小数的前提下依旧为0.000，因此它除以2后一定也为0.000，我们可以拒绝原假设，职场上男女薪资存在不平等现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 交互效应模型——定性变量间的交互效应**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· Example7.** 承接example.6，我们除了考虑性别的影响外，还决定同时考察婚姻状况对薪资的影响。考虑一下模型\n",
    "$$\n",
    "\\log (\\text { wage })=\\beta_{0}+\\delta_{0} \\text { female }+\\gamma_{0} \\text { married }+\\beta_{1} \\text { educ }+\\beta_{2} \\text { exper }+\\beta_{3} \\text { exper }{ }^{2}+\\beta_{4} \\text { tenure }+\\beta_{5} \\text { tenure }{ }^{2}+u\n",
    "$$\n",
    "在这个模型中，人群被分为四个类别：单身男性、单身女性、已婚男性、已婚女士。他们在薪资上的区别依旧可以用回归函数表示出来\n",
    "$$\n",
    "\\begin{gathered}\n",
    "E(\\log (\\text { wage }) \\mid \\text { male }, \\text { single }, x)=h(x) \\\\\n",
    "E(\\log (\\text { wage }) \\mid \\text { female, single }, x)=\\delta_{0}+h(x) \\\\\n",
    "E(\\log (\\text { wage }) \\mid \\text { male }, \\text { married }, x)=\\gamma_{0}+h(x) \\\\\n",
    "E(\\log (\\text { wage }) \\mid \\text { female }, \\text { married }, x)=\\delta_{0}+\\gamma_{0}+h(x)\n",
    "\\end{gathered}\n",
    "$$\n",
    "其中，$h(x)$在这里表示模型中不含定性变量的部分。\n",
    "\n",
    "我们可以清楚地看到，不论是未婚还是已婚，性别差异都是$\\delta_{0}$；不论是男性还是女性，结婚与否的差异都是$\\gamma_{0}$。大家稍加思考一下可能可以发现，这里面暗示着这两个定性因素彼此互不相关。在这个模型下，男性结婚与否的差异，与女性结婚与否的差异是相同的。\n",
    "\n",
    "但在现实中，这一假设未必成立。相对于男性而言，婚姻给女性在职场上带来的影响可能相对较大，这意味着这两种定性因素相互之间存在交互效应。而要在模型中体现出这种交互效应，我们需要在**原模型**的基础上加上它们的交互乘积项\n",
    "$$\n",
    "\\log (\\text { wage })=\\beta_{0}+\\delta_{0} \\text { female }+\\gamma_{0} \\text { married }+ \\delta_{\\gamma} \\text { female } * \\text { married }+\\beta_{1} \\text { educ }+\\beta_{2} \\text { exper }+\\beta_{3} \\text { exper }{ }^{2}+\\beta_{4} \\text { tenure }+\\beta_{5} \\text { tenure }{ }^{2}+u\n",
    "$$\n",
    "在这个模型下，单身男性、单身女性、已婚男性、已婚女士的薪资区别就变成了\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "E(\\log (\\text { wage }) \\mid \\text { male, single }, x)=h(x) \\\\\n",
    "E(\\log (\\text { wage }) \\mid \\text { female single, } x)=\\delta_{0}+h(x) \\\\\n",
    "E(\\log (\\text { wage }) \\mid \\text { male, married, } x)=\\gamma_{0}+h(x) \\\\\n",
    "E(\\log (\\text { wage }) \\mid \\text { female, married }, x)=\\delta_{0}+\\gamma_{0}+\\delta_{y}+h(x)\n",
    "\\end{array}\n",
    "$$\n",
    "于男性而言，婚姻差异为$\\gamma_{0}$，而对于女性而言，婚姻带来的影响是$\\gamma_{0}+\\delta_{\\gamma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  lwage   R-squared:                       0.461\n",
      "Model:                            OLS   Adj. R-squared:                  0.453\n",
      "Method:                 Least Squares   F-statistic:                     55.25\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           1.28e-64\n",
      "Time:                        15:37:19   Log-Likelihood:                -250.96\n",
      "No. Observations:                 526   AIC:                             519.9\n",
      "Df Residuals:                     517   BIC:                             558.3\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept               0.3214      0.100      3.213      0.001       0.125       0.518\n",
      "female                 -0.1104      0.056     -1.980      0.048      -0.220      -0.001\n",
      "married                 0.2127      0.055      3.842      0.000       0.104       0.321\n",
      "educ                    0.0789      0.007     11.787      0.000       0.066       0.092\n",
      "I(female * married)    -0.3006      0.072     -4.188      0.000      -0.442      -0.160\n",
      "exper                   0.0268      0.005      5.112      0.000       0.017       0.037\n",
      "I(exper ** 2)          -0.0005      0.000     -4.847      0.000      -0.001      -0.000\n",
      "tenure                  0.0291      0.007      4.302      0.000       0.016       0.042\n",
      "I(tenure ** 2)         -0.0005      0.000     -2.306      0.022      -0.001   -7.89e-05\n",
      "==============================================================================\n",
      "Omnibus:                       15.526   Durbin-Watson:                   1.785\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               32.182\n",
      "Skew:                          -0.062   Prob(JB):                     1.03e-07\n",
      "Kurtosis:                       4.205   Cond. No.                     5.06e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.06e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "wage1_lm2=sm.formula.ols('lwage~female+married+educ+I(female*married)+exper+I(exper**2)+tenure+I(tenure**2)',data=wage1).fit()\n",
    "print(wage1_lm2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p值显示，交互项十分显著，性别与婚姻状况确实存在交互效应；再看看系数：于男性而言，已婚人士平均工资比维护人士高约21.2%，但是对于女性而言，已婚人士比未婚人士工资低8.8%，可见结婚对男性和女性的影响非常不一致！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 交互效应模型——定性与定量变量的交互效应**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们进一步讨论交互效应模型，交互效应不仅可以存在于定性变量之间，也可以存在定性与定量变量之间。我们依旧用例子进行学习的导入。\n",
    "\n",
    "**· Example8.** 在example.6的模型中，female组与male组的回归函数是两条**平行的**直线，即它们的斜率——教育程度$educ$对薪资$wage$的偏效应是恒定的，这意味着性别不会对教育的边际回报产生影响。而如果我们想在模型体现或验证两者会相互影响，则可以在原模型中加入交互项$female*educ$\n",
    "$$\n",
    "\\text { wage }=\\beta_{0}+\\delta_{0} \\text { female }+\\beta_{1} \\text { educ }+\\delta_{1} \\text { female } * \\text { educ }+u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是，男性与女性的薪资差异如下\n",
    "$$\n",
    "E(\\text { wage|male }, e d u c)=\\beta_{0}+\\beta_{1} e d u c\n",
    "$$\n",
    "$$\n",
    "E(\\text { wage|female }, e d u c)=\\beta_{0}+\\delta_{0}+\\left(\\beta_{1}+\\delta_{1}\\right) e d u c\n",
    "$$\n",
    "比起之前只有截距上的不同，这里两个回归函数的斜率也不同了。\n",
    "\n",
    "对于这个模型，我们可以做两种假设检验：\n",
    "\n",
    "1、检验男性与女性的边际教育回报是否相同。这等价于检验假设\n",
    "$$\n",
    "H_{0}: \\delta_{1}=0 \\leftrightarrow H_{1}: \\delta_{1} \\neq 0\n",
    "$$\n",
    "2、检验男性与女性的平均工资是否存在性别差异。这等价于检验假设\n",
    "$$\n",
    "H_{0}: \\delta_{0}=\\delta_{1}=0 \\leftrightarrow H_{1}: \\exists \\delta_{j} \\neq 0, j=0,1\n",
    "$$\n",
    "显然，对于第一个问题，我们可以使用t检验；对于第二个问题，我们可以使用F检验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  lwage   R-squared:                       0.441\n",
      "Model:                            OLS   Adj. R-squared:                  0.433\n",
      "Method:                 Least Squares   F-statistic:                     58.37\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           1.67e-61\n",
      "Time:                        15:37:19   Log-Likelihood:                -260.49\n",
      "No. Observations:                 526   AIC:                             537.0\n",
      "Df Residuals:                     518   BIC:                             571.1\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept            0.3888      0.119      3.276      0.001       0.156       0.622\n",
      "female              -0.2268      0.168     -1.354      0.176      -0.556       0.102\n",
      "educ                 0.0824      0.008      9.725      0.000       0.066       0.099\n",
      "I(female * educ)    -0.0056      0.013     -0.426      0.670      -0.031       0.020\n",
      "exper                0.0293      0.005      5.886      0.000       0.020       0.039\n",
      "I(exper ** 2)       -0.0006      0.000     -5.398      0.000      -0.001      -0.000\n",
      "tenure               0.0319      0.007      4.647      0.000       0.018       0.045\n",
      "I(tenure ** 2)      -0.0006      0.000     -2.509      0.012      -0.001      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                       13.302   Durbin-Watson:                   1.795\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               24.887\n",
      "Skew:                          -0.080   Prob(JB):                     3.94e-06\n",
      "Kurtosis:                       4.053   Cond. No.                     8.41e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.41e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# 直接根据报告表做t检验\n",
    "wage1_lm3=sm.formula.ols('lwage~female+educ+I(female*educ)+exper+I(exper**2)+tenure+I(tenure**2)',data=wage1).fit()\n",
    "print(wage1_lm3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_resid</th>\n",
       "      <th>ssr</th>\n",
       "      <th>df_diff</th>\n",
       "      <th>ss_diff</th>\n",
       "      <th>F</th>\n",
       "      <th>Pr(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520.0</td>\n",
       "      <td>93.911297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>518.0</td>\n",
       "      <td>82.921609</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.989688</td>\n",
       "      <td>34.325541</td>\n",
       "      <td>1.002351e-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   df_resid        ssr  df_diff    ss_diff          F        Pr(>F)\n",
       "0     520.0  93.911297      0.0        NaN        NaN           NaN\n",
       "1     518.0  82.921609      2.0  10.989688  34.325541  1.002351e-14"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用anova函数做F检验\n",
    "wage1_lm3_r=sm.formula.ols('lwage~educ+exper+I(exper**2)+tenure+I(tenure**2)',data=wage1).fit()\n",
    "anova_lm(wage1_lm3_r,wage1_lm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t检验结果表明，两性之间教育边际回报相等的假设是不能被拒绝，即两性边际回报可以认为是相等的；但是F检验结果是非常显著的，即两性存在薪资差异。这可能能说明引起两性收入不平等的原因并非来自教育程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 多分类变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于二分类变量，多分类变量在现实中更常见。如季节变量（春、夏、秋、冬）；地理位置变量（华南、华北、华东、中部、西部）等。\n",
    "\n",
    "与此同时，多分类变量相比于二分类变量也更难处理，我们不能用一个变量的多个取值来定义多分类虚拟变量。如，我们不可以定义季节变量$season$的取值1/2/3/4为春/夏/秋/冬，因为这意味着不同分类之间的差异完全取决于取值之间的差！\n",
    "\n",
    "正确做法是，用多个二值虚拟变量来表示多分类定性变量。**具体的，如果一个变量有n个类别，则需要定义n-1个虚拟变量表示它**。以季节变量为例，我们定义三个虚拟变量：spring/summer/fall，当它们其中之一等于1时，代表季节为它们本身；而如果它们全都为0，则代表季节为winter。\n",
    "\n",
    "**· 虚拟变量陷阱——完全共线性**\n",
    "\n",
    "之所以需要这样定义多分类定性变量，是因为如果我们如果将winter也纳入模型中时，这四个变量会满足一个恒等关系式\n",
    "$$\n",
    "spring+summer+fall+winter=1\n",
    "$$\n",
    "这说明这四个自变量存在完全共线性，违背了CLM假设中的MLR.4，使得模型完全失效。\n",
    "\n",
    "接下来，我们对一个含有多分类变量的实例使用python进行回归，在本次python实现中你将会学习：\n",
    "\n",
    "1. 如何将一个多分类变量“分解”为多个0-1虚拟变量\n",
    "2. 分解成多个虚拟0-1变量后，如何用这些变量进行回归而不踩雷。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· Example9.** 我们对某大公司的计算机专业人员进行薪水调查，调查的目的是识别和量化哪些影响薪水差异的因素，数据中的变量描述如下：\n",
    "\n",
    "S：年薪，单位是美元；\n",
    "<br>\n",
    "X：工作经验，单位是年；\n",
    "<br>\n",
    "E：教育，1表示高中毕业，2表示获得学士学位，3表示更高学位；\n",
    "<br>\n",
    "M：1表示为管理人员，0表示非管理人员；\n",
    "\n",
    "我们以S为因变量，以X/E/M为自变量进行多元回归。其中：X为定量变量，M为二分类变量（且已经0-1化），它们已经可以直接进行回归处理了。但是E则需要进行0-1处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>X</th>\n",
       "      <th>E</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13876</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11608</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18701</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11283</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11767</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S  X  E  M\n",
       "0  13876  1  1  1\n",
       "1  11608  1  3  0\n",
       "2  18701  1  3  1\n",
       "3  11283  1  2  0\n",
       "4  11767  1  3  0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_table('./data/P130.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们对E进行虚拟变量编码，这也叫One-hot编码。我们使用pandas包的get_dummies函数进行重编码。\n",
    "\n",
    "在对E进行重编码前，我们先花一点时间介绍一下get_dummies这一函数，让大家明白这个函数的工作原理。该函数会自动变换所有具有对象类型（如字符串）的列，但是如果某列的变量是数值型变量（哪怕它实际上是分类变量），它将不会为该列创建虚拟变量，除非我们将该列的数据类型从数值转化为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interger_Feature</th>\n",
       "      <th>Categorical_Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>box</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Interger_Feature Categorical_Feature\n",
       "0                 0               socks\n",
       "1                 1                 fox\n",
       "2                 2               socks\n",
       "3                 1                 box"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建一个dataframe，它包含一个整数特征与分类字符串特征\n",
    "demo_df=pd.DataFrame({'Interger_Feature':[0,1,2,1],'Categorical_Feature':['socks','fox','socks','box']})\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interger_Feature</th>\n",
       "      <th>Categorical_Feature_box</th>\n",
       "      <th>Categorical_Feature_fox</th>\n",
       "      <th>Categorical_Feature_socks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Interger_Feature  Categorical_Feature_box  Categorical_Feature_fox  \\\n",
       "0                 0                        0                        0   \n",
       "1                 1                        0                        1   \n",
       "2                 2                        0                        0   \n",
       "3                 1                        1                        0   \n",
       "\n",
       "   Categorical_Feature_socks  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# 使用get_dummies函数\n",
    "display(pd.get_dummies(demo_df))\n",
    "print(type(demo_df.Interger_Feature[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，经过变换后，Interger Feature列由于取值的数值类型是int64（数值型的一种），因此它没有被“分解”为多个虚拟变量。如果我们想对其进行编码，需要将它的变量类型转化为字符串str。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorical_Feature</th>\n",
       "      <th>Interger_Feature_0</th>\n",
       "      <th>Interger_Feature_1</th>\n",
       "      <th>Interger_Feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>socks</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fox</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>socks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>box</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categorical_Feature  Interger_Feature_0  Interger_Feature_1  \\\n",
       "0               socks                   1                   0   \n",
       "1                 fox                   0                   1   \n",
       "2               socks                   0                   0   \n",
       "3                 box                   0                   1   \n",
       "\n",
       "   Interger_Feature_2  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   1  \n",
       "3                   0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df['Interger_Feature']=demo_df['Interger_Feature'].astype(str)\n",
    "\n",
    "pd.get_dummies(demo_df,columns=['Interger_Feature']) # 指定columns参数，就可以对我们想要虚拟变量化的列进行精准转换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_dummies函数的基本使用方法已经介绍完毕，我们开始对E进行重编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>X</th>\n",
       "      <th>M</th>\n",
       "      <th>E_1</th>\n",
       "      <th>E_2</th>\n",
       "      <th>E_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13876</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11608</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18701</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11283</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S  X  M  E_1  E_2  E_3\n",
       "0  13876  1  1    1    0    0\n",
       "1  11608  1  0    0    0    1\n",
       "2  18701  1  1    0    0    1\n",
       "3  11283  1  0    0    1    0\n",
       "4  11767  1  0    0    0    1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['E']=data['E'].astype(str)\n",
    "data_dummies=pd.get_dummies(data,columns=['E'])\n",
    "data_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，我们已经将变量E转化为了三个虚拟变量，现在可以进行回归了。注意：不可以直接将三个虚拟变量同时纳入回归当中，我们可以选取一个虚拟变量作为“基组”，然后将其他非基组的虚拟变量纳入回归。这里我们选取高中教育水平$E=1$作为基组，则方程可以这样构建\n",
    "$$\n",
    "S=\\beta_{0}+\\beta_{1} X+\\gamma_{2} E_{2}+\\gamma_{3} E_{3}+\\delta_{1} M+u\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      S   R-squared:                       0.957\n",
      "Model:                            OLS   Adj. R-squared:                  0.953\n",
      "Method:                 Least Squares   F-statistic:                     226.8\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           2.23e-27\n",
      "Time:                        15:37:21   Log-Likelihood:                -381.63\n",
      "No. Observations:                  46   AIC:                             773.3\n",
      "Df Residuals:                      41   BIC:                             782.4\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   8035.5976    386.689     20.781      0.000    7254.663    8816.532\n",
      "X            546.1840     30.519     17.896      0.000     484.549     607.819\n",
      "E_2         3144.0352    361.968      8.686      0.000    2413.025    3875.045\n",
      "E_3         2996.2103    411.753      7.277      0.000    2164.659    3827.762\n",
      "M           6883.5310    313.919     21.928      0.000    6249.559    7517.503\n",
      "==============================================================================\n",
      "Omnibus:                        2.293   Durbin-Watson:                   2.237\n",
      "Prob(Omnibus):                  0.318   Jarque-Bera (JB):                1.362\n",
      "Skew:                          -0.077   Prob(JB):                        0.506\n",
      "Kurtosis:                       2.171   Cond. No.                         33.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "data_lm=sm.formula.ols('S~X+E_2+E_3+M',data=data_dummies).fit()\n",
    "print(data_lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 带有自变量函数的回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓自变量函数，其实就是将原自变量进行一种变换。变换后的变量一方面可能使模型对数据的拟合效果更佳；另一方面也可以满足一些我们实际的数据分析需求。在这一章节，我们简单介绍三种常见的自变量函数：对数化、二次项化、交互项化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 对数化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 对数化变量系数的解释**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们考虑一般地模型\n",
    "$$\n",
    "\\log (y)=\\beta_{0}+\\beta_{1} \\log \\left(x_{1}\\right)+\\beta_{2} x_{2}+u\n",
    "$$\n",
    "对于因变量与自变量同为对数的系数，其满足如下公式\n",
    "$$\n",
    "\\beta _1\\approx \\frac{\\Delta y/y}{\\Delta x_1/x_1}\\Rightarrow \\frac{\\Delta y}{y}\\approx \\beta _1\\frac{\\Delta x_1}{x_1}\n",
    "$$\n",
    "约等号两边都以增量比例的形式代表增量，于是我们可以将之解读为：在其他因素不变的条件下，$x_1$每增加1\\%，$y$会增加$\\beta_1\\%$。\n",
    "\n",
    "而对于因变量与自变量一个为对数一个不为对数的系数，以上述模型$\\beta_2$为例，其满足如下公式\n",
    "$$\n",
    "\\beta _2\\approx \\frac{\\Delta y/y}{\\Delta x_2}\\Rightarrow \\frac{\\Delta y}{y}\\approx \\beta _2\\Delta x_2\n",
    "$$\n",
    "约等号一边为纯增量，一边为增量比例的形式，对于该模型我们可以将之解读为：在其他因素不变的条件下，$x_2$每增加1个单位，$y$会增加$\\beta_2\\%$。\n",
    "\n",
    "严格地说，对数化变量系数的解读并没有上面这么简单，上面的解读方法只是一种近似，但是一般情况下我们不用考虑得那么严格。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 对数变换的作用——一些经验主义**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数变换是线性回归中非常常见的变量变换，它的作用非常明显：\n",
    "\n",
    "1. 正如上面的例子所示，对数变换可以方便地计算变换百分比，于“价格”型变量而言，百分比解释比绝对值解释更有经济意义。\n",
    "2. 当**因变量**为严格取正的变量，它的分布一般存在异方差性或偏态性，这容易违背CLM假设的同方差/正态性假设。而对数变换可以缓和这种情况。\n",
    "\n",
    "然而，对数变换并不能滥用，因为在一些情况下对数变换会产生极端值。首先，存在负值的变量不可以对数变换；其次，当原变量$y$有部分取值位于[0,1]区间时，$log(y)$的负数值会非常大！而线性模型对极端值是非常敏感的，这会影响模型的效果。\n",
    "\n",
    "对于变量何时取对数，没有一个准确的标准，但在长久的实践中，我们认为可以遵循以下经验：\n",
    "\n",
    "1. 对于大数值大区间变量（价格类变量、人口变量等），可取对数变换，如：工资、薪水、销售额、企业市值、人口数量、雇员数量等。\n",
    "2. 对于小数值小区间变量（时间类变量等），一般不取对数变换，如：受教育年限、工作年限、年龄等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'lprice'}>]], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 对数化对正态化的作用\n",
    "## 以hprice1的price为例\n",
    "### 未经对数化的直方图\n",
    "hprice1.hist(column='price',figsize=(8,6))\n",
    "\n",
    "### 对数化后的直方图\n",
    "hprice1.hist(column='lprice',figsize=(8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，对数化后的数据分布更接近正态分布！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 二次项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "老样子，一个例子讲明白二次项的作用！\n",
    "\n",
    "**· Example10.** 直觉上看，职场里资历越老的人工资水平就越高，于是我们想知道某公司内员工工资水平与单位工作年限之前的关系。很自然地，我们会把模型简单地假设为\n",
    "$$\n",
    "wage=\\beta _0+\\beta _1exper+u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，这个模型有一个暗含的前提：随着工作年限的提升，每年平均工资上升的幅度都相同。但现实中万恶的资本家不会那么便宜我们的，经验告诉我们，随着工作年限的上升，工资提升会逐渐变缓。也就是说，**工作年限**对工资水平的影响可能不是线性的，而是有一个“弧度”。如何让模型具备这个“弧度”呢？最简单也是最直观的方法就是，**在原模型的基础上**加入二次项\n",
    "$$\n",
    "wage=\\beta _0+\\beta _1exper+\\beta _2exper^2+u\n",
    "$$\n",
    "假设经过OLS估计后，模型为\n",
    "$$\n",
    "\\widehat{\\text { wage }}=3.73+0.298 \\text { exper }-0.006 \\text { exper }^{2}\n",
    "$$\n",
    "第一年工资增加0.298单位，第二年增加0.286单位，第十年只能增加0.178单位，二次项验证了我们的猜想。\n",
    "\n",
    "总结：如果我们在回归建模前认为某个自变量对因变量的影响不是线性的，可以尝试加入二次项，并观察二次项的显著性。如果显著，就说明两者关系确实为非线性的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 交互项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在定性变量章节介绍了定性变量之间以及定性与定量变量之间交互项的理解与意义，在这一小节我们介绍定量变量之间的交互项的理解。\n",
    "\n",
    "定量变量间交互项的意义和之前的一样，若某一个变量对因变量的效应依赖于另一个自变量，则我们可以在模型中加入交互项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· Example11.** 我们用出勤百分比atndrte，读大学前的GPA、ACT来解释一批学生期末考试标准化的成绩。有人认为对于过去成绩不同的学生，出勤率可能具有不同的影响，于是我们在模型中加入两者的交互项\n",
    "$$\n",
    "\\,\\,\\mathrm{stndfnl} =\\beta _0+\\beta _1\\,\\,\\mathrm{atndrte} +\\beta _2\\,\\,\\mathrm{priGP} A+\\beta _3ACT+\\beta _4priGPA^2+\\beta _5ACT^2+\\beta _6priGPA\\cdot \\,\\,\\mathrm{atndrte} +u\n",
    "\\\\\n",
    "$$\n",
    "在这里我们想提醒一下大家，在没有学习交互项、二次项的时候，我们会说自变量$x_i$的系数$\\beta_i$是其对因变量$y$的偏效应，这是因为在全为一次项的模型中，$\\frac{\\Delta y}{\\Delta x_i}$就等于系数$\\beta_i$。而加入了交互项、二次项后，我们对存在交互项、二次项的自变量的系数解释就不会那么简单了。我们更关注问题的本质——$x_i$对因变量$y$的偏效应是多少，对于本例而言，出勤率对成绩的偏效应如下所示\n",
    "$$\n",
    "\\frac{\\Delta \\text { stndfnl }}{\\Delta a t n d r t e}=\\beta_{1}+\\beta_{6} \\text { priGPA }\n",
    "$$\n",
    "可见，$priGPA$在不同取值下，出勤率对成绩的影响是不一样的。其实有了这个公式，我们可以计算出$priGPA$在不同取值下，出勤率对成绩偏效应的具体值，这一任务非常简单。但是涉及交互问题的假设检验就相对复杂一点，也非常有趣。通常来说，这类问题假设检验有如下两个\n",
    "\n",
    "1. 出勤率对成绩的影响是否显著？对于这一问题，假设需要设置成这样\n",
    "$$\n",
    "H_0: \\beta _1=\\beta _6=0 \\leftrightarrow \\,\\,H_1: H_0\\text{不成立}\n",
    "$$\n",
    "很明显，我们用F检验即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attend</th>\n",
       "      <th>termgpa</th>\n",
       "      <th>priGPA</th>\n",
       "      <th>ACT</th>\n",
       "      <th>final</th>\n",
       "      <th>atndrte</th>\n",
       "      <th>hwrte</th>\n",
       "      <th>frosh</th>\n",
       "      <th>soph</th>\n",
       "      <th>skipped</th>\n",
       "      <th>stndfnl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>2.64</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>84.375</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.472689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.52</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>68.750</td>\n",
       "      <td>87.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.052521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.46</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>93.750</td>\n",
       "      <td>87.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.0</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.61</td>\n",
       "      <td>20.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>96.875</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.262605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.32</td>\n",
       "      <td>23.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.733193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attend  termgpa  priGPA   ACT  final  atndrte  hwrte  frosh  soph  skipped  \\\n",
       "0    27.0     3.19    2.64  23.0   28.0   84.375  100.0    0.0   1.0      5.0   \n",
       "1    22.0     2.73    3.52  25.0   26.0   68.750   87.5    0.0   0.0     10.0   \n",
       "2    30.0     3.00    2.46  24.0   30.0   93.750   87.5    0.0   0.0      2.0   \n",
       "3    31.0     2.04    2.61  20.0   27.0   96.875  100.0    0.0   1.0      1.0   \n",
       "4    32.0     3.68    3.32  23.0   34.0  100.000  100.0    0.0   1.0      0.0   \n",
       "\n",
       "    stndfnl  \n",
       "0  0.472689  \n",
       "1  0.052521  \n",
       "2  0.892857  \n",
       "3  0.262605  \n",
       "4  1.733193  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attend=pd.read_stata('./data/attend.dta')\n",
    "attend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_resid</th>\n",
       "      <th>ssr</th>\n",
       "      <th>df_diff</th>\n",
       "      <th>ss_diff</th>\n",
       "      <th>F</th>\n",
       "      <th>Pr(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>675.0</td>\n",
       "      <td>519.343813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>673.0</td>\n",
       "      <td>512.762440</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.581373</td>\n",
       "      <td>4.319021</td>\n",
       "      <td>0.013684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   df_resid         ssr  df_diff   ss_diff         F    Pr(>F)\n",
       "0     675.0  519.343813      0.0       NaN       NaN       NaN\n",
       "1     673.0  512.762440      2.0  6.581373  4.319021  0.013684"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果显著，可以拒绝原假设\n"
     ]
    }
   ],
   "source": [
    "# 构建有约束、无约束模型\n",
    "attend_lm_ur=sm.formula.ols('stndfnl~atndrte+priGPA+ACT+I(priGPA**2)+I(ACT**2)+I(priGPA*atndrte)',data=attend).fit()\n",
    "attend_lm_r=sm.formula.ols('stndfnl~priGPA+ACT+I(priGPA**2)+I(ACT**2)',data=attend).fit()\n",
    "\n",
    "# F检验\n",
    "display(anova_lm(attend_lm_r,attend_lm_ur))\n",
    "print('结果显著，可以拒绝原假设')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 当$priGPA$为一个具体的值时，出勤率在这个值下对对成绩的影响是否显著？\n",
    "\n",
    "这个问题非常有趣，要回答这个问题，我们需要对原模型做一些小改变。假设我们想验证$priGPA=2.59$时出勤率的显著性，我们需要将原来的$atndrte$修改为$atndrte-2.59$。模型也将变成\n",
    "$$\n",
    "\\,\\,\\mathrm{stndfnl}=\\beta _0+\\beta _1\\,\\,\\mathrm{atndrte}+\\beta _2\\,\\left( GPA-2.59 \\right) +\\beta _3ACT+\\beta _4\\left( priGPA-2.59 \\right) ^2+\\beta _5ACT^2+\\beta _6\\left( priGPA-2.59 \\right) \\cdot \\,\\,\\mathrm{atndrte}+u\n",
    "$$\n",
    "我们根据这个模型求偏导会有\n",
    "$$\n",
    "\\frac{\\Delta \\text { stndfnl }}{\\Delta a t n d r t e}=\\beta_{1}+\\beta_{6} \\text { (priGPA-2.59) }\n",
    "$$\n",
    "当$atndrte=2.59$时，偏效应正好就是$\\beta_1$！我们只需要查看这个模型的$\\beta_1$显著性就可以回答这一假设检验问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                stndfnl   R-squared:                       0.229\n",
      "Model:                            OLS   Adj. R-squared:                  0.222\n",
      "Method:                 Least Squares   F-statistic:                     33.25\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           3.49e-35\n",
      "Time:                        15:37:22   Log-Likelihood:                -868.90\n",
      "No. Observations:                 680   AIC:                             1752.\n",
      "Df Residuals:                     673   BIC:                             1783.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -0.1827      1.128     -0.162      0.871      -2.397       2.031\n",
      "atndrte                  0.0078      0.003      2.938      0.003       0.003       0.013\n",
      "priGPA2                 -0.0958      0.367     -0.261      0.794      -0.816       0.624\n",
      "ACT                     -0.1280      0.098     -1.300      0.194      -0.321       0.065\n",
      "I(priGPA2 ** 2)          0.2959      0.101      2.928      0.004       0.097       0.494\n",
      "I(ACT ** 2)              0.0045      0.002      2.083      0.038       0.000       0.009\n",
      "I(priGPA2 * atndrte)     0.0056      0.004      1.294      0.196      -0.003       0.014\n",
      "==============================================================================\n",
      "Omnibus:                        2.581   Durbin-Watson:                   2.279\n",
      "Prob(Omnibus):                  0.275   Jarque-Bera (JB):                2.474\n",
      "Skew:                          -0.095   Prob(JB):                        0.290\n",
      "Kurtosis:                       3.226   Cond. No.                     1.85e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.85e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "结果显著，可以拒绝原假设\n"
     ]
    }
   ],
   "source": [
    "attend['priGPA2']=attend['priGPA']-2.59\n",
    "attend_lm2=sm.formula.ols('stndfnl~atndrte+priGPA2+ACT+I(priGPA2**2)+I(ACT**2)+I(priGPA2*atndrte)',data=attend).fit()\n",
    "print(attend_lm2.summary())\n",
    "print('结果显著，可以拒绝原假设')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CLM假设误差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结束了前四章的学习，大家基本上掌握了回归分析中比较重要的知识了。我们简单回顾一下前面所学内容：\n",
    "\n",
    "1. 第一章，我们学习了回归的基本思想，并介绍了本课程中最重要、最常用的模型——多元线性回归模型。\n",
    "2. 第二章，我们介绍了多元线性回归模型的基本六大假设——CLM假设，OLS最小二乘估计法并学习了在这套假设下的性质。需要注意的是，后面两章的内容均建立这些OLS性质上，我们需要时刻牢记这一点。\n",
    "3. 第三章，我们学习了回归分析中最重要的分析任务之一——模型的统计推断，介绍了回归分析中各种类型的假设检验。\n",
    "4. 第四章，我们对线性模型进行了拓展，将回归元从定量变量一次项拓展至定性变量、对数项、二次项、交互项，使模型蕴含的信息更丰富。\n",
    "\n",
    "然而，以上内容均建立在模型严格满足CLM假设的前提下，一旦模型违背了CLM假设中的某一条假设，模型估计的精度、假设检验的可信度将受到影响。那么，违反某条CLM假设具体会给模型估计带来多大的影响呢？这就是本节需要讨论的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 OLS估计另一种更为常用的求解方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在前面的章节中给出了OLS估计系数求解的矩阵表达式\n",
    "$$\n",
    "\\hat{\\beta}=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} y\n",
    "$$\n",
    "并给出了每个估计系数$\\hat{\\beta_j}$估计标准误的矩阵表达式\n",
    "$$\n",
    "\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)=\\hat{\\sigma} \\sqrt{\\left(X^{\\prime} X\\right)_{j+1, j+1}^{-1}}\n",
    "$$\n",
    "矩阵表达式的一大好处就是方便求解，直接矩阵运算就完事了。但是这并不利于我们观察它们的“构成”——如果我想知道系数的估计标准误受什么因素影响，并从此获知降低误差的方法，那么面对上面这个只有设计阵$X$的公式，我们只能阿巴阿巴。\n",
    "\n",
    "好在OLS估计确实有另一种求法，且这种求法得来的公式可以满足我们这一需求，接下来就让我们简单介绍一下这种求法，并用它来总结出系数估计方差的影响因素吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 解法步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该部分仅作了解即可。\n",
    "\n",
    "我们以求回归模型\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "中$\\beta_1$为例，大体上可分为两步\n",
    "\n",
    "**· step 1**，以$x_1$做因变量，对其他自变量做回归\n",
    "$$\n",
    "x_{1}=\\alpha_{0}+\\alpha_{2} x_{2}+\\cdots+\\alpha_{k} x_{k}+r_{1}\n",
    "$$\n",
    "记残差为$\\hat{r}_{1}=\\left(\\hat{r}_{11}, \\hat{r}_{21}, \\cdots, \\hat{r}_{n 1}\\right)^{\\prime}$\n",
    "\n",
    "**· step 2**，$y$对残差$\\hat{r}_{1}$做过原点（无截距）的回归\n",
    "$$\n",
    "y=\\gamma \\hat{r}_{1}+u\n",
    "$$\n",
    "可以证明，$\\beta_1$的估计就是$\\hat{y}$\n",
    "$$\n",
    "\\hat{\\gamma}=\\hat{\\beta}_1=\\frac{\\sum_{i=1}^n{\\hat{r}_{i1}}y_i}{\\sum_{i=1}^n{\\hat{r}_{i1}^{2}}}\n",
    "$$\n",
    "我们仔细品味一下$\\beta_1$的求解过程，会发现它非常符合我们对回归模型系数的理解——在第一步的自变量回归中，残差$\\hat{r}_{1}$相当于是在$x_1$中排除了$x_2$,……,$x_n$影响后的剩余部分，因此$\\hat{\\beta}_1$相当于在度量排除了其余自变量影响后，$x_1$对$y$的影响。而我们回想一下之前我们对$\\beta_1$的解释有一个重要的前置语：“在其他自变量不变的前提下”，这恰恰就是排除其他变量的影响的含义。\n",
    "\n",
    "我们用python实操一波，看看这种求解方法的结果和之前的求解方法是否一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "常规解法的结果:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.286328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.009426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsGPA</th>\n",
       "      <td>0.453456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params\n",
       "const  1.286328\n",
       "ACT    0.009426\n",
       "hsGPA  0.453456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "新解法的效果\n",
      "0.009426012260473621\n"
     ]
    }
   ],
   "source": [
    "# 以gpa_lm2的ACT系数为例\n",
    "\n",
    "# 常规解法的结果\n",
    "print('常规解法的结果:')\n",
    "display(c2)\n",
    "print('-------------------------------------------')\n",
    "\n",
    "# 新求解法\n",
    "gpa_lm2_pre=sm.formula.ols('ACT~hsGPA',data=gpa1).fit()\n",
    "gpa1['resid']=gpa_lm2_pre.resid\n",
    "gpa_lm2_pre2=sm.formula.ols('colGPA~resid-1',data=gpa1).fit()\n",
    "print('新解法的效果')\n",
    "print(gpa_lm2_pre2.params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，两种解法是完全等价的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 估计系数的方差构成与多重共线性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1~MLR.5下，$\\operatorname{Var}\\left(\\hat{\\beta}_{1}\\right)$可计算为\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{1}\\right)=\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} \\hat{r}_{i 1}^{2}}=\\frac{\\sigma^{2}}{S S T_{1}\\left(1-R_{1}^{2}\\right)}\n",
    "$$\n",
    "其中，$S S T_{1}=\\sum_{i=1}^{n}\\left(x_{i 1}-\\bar{x}_{1}\\right)^{2}$为$x_1$变量本身的完全平方和，$R_{1}^{2}$是$x_1$对其他解释变量所作回归的拟合优度判决系数，$\\sigma^{2}$则是之前提及的随机误差的方差。至此，回归系数估计方差的构成就结构完毕了，它依赖于三个因素：\n",
    "\n",
    "**· 误差方差$\\sigma^{2}$**，随机误差的方差可以理解为数据集的噪声信息，噪声信息越多，估计的不确定性就越大，方差就越大，这点很顺理成章！但是一般来说，对于给定的数据集，误差方差也是随之确定的，我们一般不会在这个因素上下文章。\n",
    "\n",
    "**· 自变量$x_1$本身的总变异$SST_1$**，自变量总变异越大，表明自变量散步程度越高，估计越牢靠。关于这个指标，我们会发现：对于一个分析任务来说，样本量越大，总变异也就越大，进而估计方差会变小，这告诉我们更多的随机样本有利于提高估计的精度！\n",
    "\n",
    "**· 自变量间的线性关联程度$R_{1}^{2}$**，$R_{1}^{2}$约接近1，$x_1$与其他自变量之间的线性关系就越强烈，估计方差也越大。这种近似的共线性关系被称为**多重共线性(multicolinearity)**，它不同于完全共线性（事实上$R_{1}^{2}=1$时就是完全共线性），这种现象在数据分析中普遍存在，只是程度有所区别。那么如何衡量共线性的严重程度呢？我们一般用方差膨胀因子(VIF,Variance Inflation Factor)来评判\n",
    "$$\n",
    "VIF_{x1}=\\frac{1}{1-R_{1}^{2}}\n",
    "$$\n",
    "若$VIR>10$，意味着共线性很严重，需要采取措施降低共线性。\n",
    "\n",
    "现在的问题是，如何降低多重共线性呢？一种方法是使变量之间尽可能的不相关，关于这点我们可以查看变量之间的皮尔逊相关系数，如果某两个变量之间的相关系数非常高，我们就需要考虑不把它们同时放入模型中；第二种方法是减少自变量的个数。一般而言，模型中自变量个数越多，R方会越大。而$R_{1}^{2}$正是$x_1$对其他自变量做回归的判决系数，显然，其他自变量越多，$R_{1}^{2}$就越大。这个方法告诉我们，**模型的自变量绝非越多越好**，有关这个问题的讨论，我们会在下一节做深入探讨。\n",
    "\n",
    "最后，我们总结一下降低估计方差，提高估计精度的方法\n",
    "\n",
    "1. 采取更合理的数据采样方法，降低数据噪声。\n",
    "2. 增大数据样本量\n",
    "3. 根据线性相关程度筛选纳入模型的变量，且切忌纳入过多变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 模型误设的误差分析——违反MLR.1的后果是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 如何理解模型误设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1中，我们假设自己设置的模型是“正确的”，即对\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "的假设上，我们正确地纳入了所有关键的自变量，且没有纳入多余的自变量。而多纳入一个无关的变量，以及少纳入一个关键的变量，都能算是违反MLR.1，我们可以看看误设模型对模型中变量系数的估计会有怎样子的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 模型误设的后果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 遗漏相关变量**\n",
    "\n",
    "假设真模型为\n",
    "$$\n",
    "y \\sim x_{1}+x_{2}+\\cdots+x_{k-1}+x_{k}\n",
    "$$\n",
    "但我们实际拟合的模型却是\n",
    "$$\n",
    "y \\sim x_{1}+x_{2}+\\cdots+x_{k-1}\n",
    "$$\n",
    "假设真模型的系数估计记为$\\hat{\\beta}_{i}$，实际模型的系数估计为$\\tilde{\\beta}_{i}$，那么有\n",
    "$$\n",
    "\\tilde{\\beta}_{1}=\\hat{\\beta}_{1}+\\hat{\\beta}_{k} \\tilde{\\delta}_{1}\n",
    "$$\n",
    "其中，$\\tilde{\\delta}_{1}$是遗漏变量对其余变量的辅助回归$x_{k} \\sim x_{1}+x_{2}+\\cdots+x_{k-1}$中$x_1$对应的回归系数。我们观察一下$\\tilde{\\beta}_{1}$的无偏性情况\n",
    "$$\n",
    "E\\left(\\tilde{\\beta}_{1} \\mid x\\right)=\\beta_{1}+\\beta_{k} \\tilde{\\delta}_{1}\n",
    "$$\n",
    "发现，原本正确模型假设下的无偏性$E\\left( \\hat{\\beta}_1 \\right) =\\beta _1$，因漏设相关变量$x_k$导致多了偏差$\\beta_{k} \\tilde{\\delta}_{1}$。\n",
    "\n",
    "因此，对于遗漏变量$x_k$，除非$\\beta_k=0$（即$x_k$对$y$完全不影响），或者$\\tilde{\\delta}_{1}=0$（遗漏变量$x_k$与待分析自变量$x_1$不相关），否则遗漏变量会对我们实际估计模型的系数估计产生有偏影响，即我们的系数估计不再是平均意义上的准确！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 多纳入一个无关变量**\n",
    "\n",
    "假设真模型为\n",
    "$$\n",
    "y \\sim x_{1}+x_{2}+\\cdots+x_{k-1}\n",
    "$$\n",
    "但实际拟合的模型是\n",
    "$$\n",
    "y \\sim x_{1}+x_{2}+\\cdots+x_{k-1}+x_{k}\n",
    "$$\n",
    "且：$x_k$与其余自变量$x_i$存在相关性，但与因变量$y$不存在相关性（即无法对因变量变异做出解释），此时，我们实际拟合的模型相当于是\n",
    "$$\n",
    "y=\\beta _0+\\beta _1x_1+\\cdots +\\beta _{k-1}x_{k-1}+0x_k+u\n",
    "$$\n",
    "这种过度设定**其实并不违反MLR.1-MLR.4中的任意一条**，因为就算将这个无关变量纳入模型，最终拟合出来的变量对应系数也会是0。因此，纳入无关变量这种情况对于建立在前四个假设上的OLS无偏性是没有影响的！即依然有\n",
    "$$\n",
    "E\\left( \\widetilde{\\beta _i} \\right) =\\beta _i\n",
    "$$\n",
    "但是只要$x_k$与其它自变量存在共线性，将其纳入会使其他系数的估计方差变大。以$x_1$的系数为例，假设纳入$x_k$前它的方差是\n",
    "$$\n",
    "\\mathrm{Var}\\left( \\hat{\\beta}_1 \\right) =\\frac{\\sigma ^2}{SST_1\\left( 1-R_{1,k-1}^{2} \\right)}\n",
    "$$\n",
    "而纳入$x_k$后它的方差就变成了\n",
    "$$\n",
    "\\mathrm{Var}\\left( \\tilde{\\beta}_1 \\right) =\\frac{\\sigma ^2}{SST_1\\left( 1-R_{1,k}^{2} \\right)}\n",
    "$$\n",
    "由于而多自变量的模型R方总是大于少自变量模型的R方，即\n",
    "$$\n",
    "R_{1,k-1}^{2}<R_{1,k}^{2}\n",
    "$$\n",
    "于是最终有\n",
    "$$\n",
    "\\mathrm{Var}\\left( \\hat{\\beta}_1 \\right) <\\mathrm{Var}\\left( \\tilde{\\beta}_1 \\right) \n",
    "$$\n",
    "因此，对于增加一个**对解释因变量无益**的自变量，一旦这个无益自变量与其它自变量存在共线性，则会增大其他估计系数的估计方差！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 变量选择的方法论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们总结一下以上的内容：\n",
    "\n",
    "1. 减少模型中的变量，可能会导致模型其他系数的估计有偏，但是会使系数估计的方差减少，即：增大偏差、减小方差\n",
    "2. 增加模型中的变量，不会导致模型系数估计有偏，但是会使系数估计的方差增大，即：减小偏差、增大方差\n",
    "\n",
    "而在实际情况中，我们不可能得知一个真正正确的模型，而是在摸索中慢慢找到适合纳入模型的变量。通常来说，有两种考量纳入变量的方法论：\n",
    "\n",
    "1. 从少数变量开始，一步步向模型纳入我们认为重要的变量，并通过t检验显著性来判断是否纳入该变量；如果我们认为变量存在二次项效应与交互项效应，可以将其纳入模型并通过联合F检验判断其显著性，直到模型的解释度达到一个较高的水平。\n",
    "2. 从多数变量开始，一步步将不显著的变量剔除出模型。\n",
    "\n",
    "这两种方法该用哪一种，视具体情况而定。假若数据集的变量非常多，那我倾向于使用第一种方法，一步步将我们认为重要的变量纳入模型；若变量不多，则第二种方法可以考虑。\n",
    "\n",
    "对于变量的选择，还有一些自动化的方法，如向前逐步选择、向后逐步选择等等。但我个人认为，这些自动的变量选择方法更关注模型的预测能力，而不注重变量本身对模型的意义，它们更适用于机器学习的预测任务，而不适用于回归分析，因为回归分析中最重要的意义就是“解释与推断”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 模型不满足正态性的分析——违反MLR.6的后果是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提前公布答案：在样本量较大的情况下，不满足MLR.6不会有什么不好的后果！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 如何理解与观测正态性假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先回顾一下MLR.6：该假设假定随机误差$u$在任何自变量$x$已知的条件下服从正态分布\n",
    "$$\n",
    "u \\mid x \\sim N\\left(0, \\sigma^{2}\\right)\n",
    "$$\n",
    "即每个样本的误差$u_1,u_2,...,u_n$都是来源于一个正态总体。然而，随机误差在数据集中是根本观测不到的，而由于在$x$已知的条件下，$u$的正态性等价于$y$的正态性，因此我们只需要查看样本因变量$y$的分布图就可以大致判断正态性了。\n",
    "\n",
    "然而，因变量$y$不为正态分布的情况是非常常见的，以下面的$narr86$为例，它统计了青年人在某一特定年份被拘捕的次数。很明显，绝大部分青年人都没有被拘捕过，因此该变量的分布是一个严重的偏态分布，不可能为正态分布。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0     1970\n",
      "1.0      559\n",
      "2.0      121\n",
      "3.0       42\n",
      "5.0       13\n",
      "4.0       12\n",
      "6.0        4\n",
      "12.0       1\n",
      "9.0        1\n",
      "10.0       1\n",
      "7.0        1\n",
      "Name: narr86, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAF1CAYAAADFgbLVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXVElEQVR4nO3df5Dc9X3f8ecryI5tRC0Y2iuWaERmVCfYJNi+AVI66alusMAZiyQdF0pBtmmVmULtZOg04E6G1DYtnRrHNnZoZKMAE8UqY+NBJYqJovpC6ZQEcCnihx1UWxgpGEGEZcswsUXe/WO/QrdCP47Tavc+d8/HzM3tfva73+9nP4N4ar/31V6qCkmSNPv92KgnIEmSpsdoS5LUCKMtSVIjjLYkSY0w2pIkNcJoS5LUCKMtSVIjjLakaUtyZpL/mWR3ku1JfvOAx9+Q5HeSPNdtc8+o5irNRQtGPQFJs0uSAKmqv5kytqCq9gJ/AHwZmACWAvcm+b9VtaHbdA29/6/8NLALOHN4M5fmPt9pS3NIkm1J/m2Sh7t3uv8tyeuSnJjkriTPJnm+u71kyvMmk1yX5H8BLwA/maSSXJHkCeCJbtOlwLqqeqmq/h9wL/CWbh8/BbwHWF1Vz3bbPDjM1y/NdUZbmnveC6wATgN+BngfvT/rvwf8BPD3gBeBzxzwvEuB1cAJwJPd2IXA2cDp3f1PApcleU2SNwM/B/xJ99hZ3fP+Q3d6fEuSXxnwa5PmNaMtzT2frqq/rKpdwH8Hzqyqv6qqL1XVC1X1feA64B8d8LxbqurRqtpbVT/qxv5TVe2qqhe7+3cB/5Re9L8O3FxV93ePLQHeCuwG3gRcCdya5KeP2SuV5hmjLc0935ly+wVgYXeB2O8meTLJ94B7gEVJjpuy7VMH2dfLY0lOAr4CfAR4HXAq8K4k/7rb5EXgR8DHquqHVfWnwFeB8wb1wqT5zmhL88NVwJuBs6vqbwE/341nyjYH+5V/U8d+Enipqm7r3o1vB9YDF3SPP3yE50s6SkZbmh9OoPdO+LvdO+ZrZ7CPv6B3cfk/T/JjSf4u8M/YH+t7gG8D1yRZkORcYDlw99FPXxIYbWm++CTweuA54D56p7lflar6HvDLwK8DzwMPAY8AH+se/xGwkt47793A54DLqurrRz17SUDv32KOeg6SJGkafKctSVIjjLYkSY0w2pIkNcJoS5LUCKMtSVIjZvVv+Tr55JNr6dKlA93nD37wA44//viB7rNVrkU/16Of67Gfa9HP9eg36PV48MEHn6uqv32wx2Z1tJcuXcoDDzww0H1OTk4yMTEx0H22yrXo53r0cz32cy36uR79Br0eSZ481GOeHpckqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFH/C1fSU4FbgPGgALWVNWnkvwW8K+AZ7tNP1xVG7vnXANcDrwEfLCq7u7GVwCfAo4DPl9V1w/25RzZlh27ed/Vfzjsw07btuvfPeopSJJmqen8as69wFVV9bUkJwAPJtnUPfbbVfXxqRsnOR24CHgL8CbgT5L8/e7hzwK/AGwH7k+yoaoeG8QLkSRprjtitKvqaeDp7vb3kzwOLD7MU1YC66vqr4FvJdkKnNU9trWqvgmQZH23rdGWJGkaXtXPtJMsBd4G/Fk3dGWSh5OsTXJiN7YYeGrK07Z3Y4calyRJ05Cqmt6GyULgT4HrquqOJGPAc/R+zv1R4JSq+kCSzwD3VdXvd8+7GfijbjcrqupfduOXAmdX1ZUHHGc1sBpgbGzsHevXrz/a19hn567dPPPiQHc5UGcsfuPQjrVnzx4WLlw4tOPNdq5HP9djP9ein+vRb9DrsXz58geravxgj03nZ9okeQ3wJWBdVd0BUFXPTHn8c8Bd3d0dwKlTnr6kG+Mw4y+rqjXAGoDx8fGamJiYzhSn7cZ1d3LDlmm97JHYdsnE0I41OTnJoNe3Za5HP9djP9ein+vRb5jrccTT40kC3Aw8XlWfmDJ+ypTNfgl4pLu9AbgoyY8nOQ1YBvw5cD+wLMlpSV5L72K1DYN5GZIkzX3Tect5LnApsCXJQ93Yh4GLk5xJ7/T4NuBXAarq0SS307vAbC9wRVW9BJDkSuBuev/ka21VPTqwVyJJ0hw3navH7wVykIc2HuY51wHXHWR84+GeJ0mSDs1PRJMkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFHjHaSU5N8NcljSR5N8qFu/KQkm5I80X0/sRtPkk8n2Zrk4SRvn7KvVd32TyRZdexeliRJc8903mnvBa6qqtOBc4ArkpwOXA1srqplwObuPsD5wLLuazVwE/QiD1wLnA2cBVy7L/SSJOnIjhjtqnq6qr7W3f4+8DiwGFgJ3NptditwYXd7JXBb9dwHLEpyCvAuYFNV7aqq54FNwIpBvhhJkuayVNX0N06WAvcAbwW+XVWLuvEAz1fVoiR3AddX1b3dY5uB3wAmgNdV1ce68d8EXqyqjx9wjNX03qEzNjb2jvXr1x/N63uFnbt288yLA93lQJ2x+I1DO9aePXtYuHDh0I4327ke/VyP/VyLfq5Hv0Gvx/Llyx+sqvGDPbZgujtJshD4EvBrVfW9Xqd7qqqSTL/+h1FVa4A1AOPj4zUxMTGI3b7sxnV3csOWab/sodt2ycTQjjU5Ocmg17dlrkc/12M/16Kf69FvmOsxravHk7yGXrDXVdUd3fAz3Wlvuu87u/EdwKlTnr6kGzvUuCRJmobpXD0e4Gbg8ar6xJSHNgD7rgBfBdw5Zfyy7iryc4DdVfU0cDdwXpITuwvQzuvGJEnSNEznPPG5wKXAliQPdWMfBq4Hbk9yOfAk8N7usY3ABcBW4AXg/QBVtSvJR4H7u+0+UlW7BvEiJEmaD44Y7e6Cshzi4XceZPsCrjjEvtYCa1/NBCVJUo+fiCZJUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktSII0Y7ydokO5M8MmXst5LsSPJQ93XBlMeuSbI1yTeSvGvK+IpubGuSqwf/UiRJmtum8077FmDFQcZ/u6rO7L42AiQ5HbgIeEv3nN9JclyS44DPAucDpwMXd9tKkqRpWnCkDarqniRLp7m/lcD6qvpr4FtJtgJndY9trapvAiRZ32372KufsiRJ89PR/Ez7yiQPd6fPT+zGFgNPTdlmezd2qHFJkjRNR3ynfQg3AR8Fqvt+A/CBQUwoyWpgNcDY2BiTk5OD2O3Lxl4PV52xd6D7HKRBv97D2bNnz1CPN9u5Hv1cj/1ci36uR79hrseMol1Vz+y7neRzwF3d3R3AqVM2XdKNcZjxA/e9BlgDMD4+XhMTEzOZ4iHduO5Obtgy07+rHHvbLpkY2rEmJycZ9Pq2zPXo53rs51r0cz36DXM9ZnR6PMkpU+7+ErDvyvINwEVJfjzJacAy4M+B+4FlSU5L8lp6F6ttmPm0JUmaf474ljPJF4AJ4OQk24FrgYkkZ9I7Pb4N+FWAqno0ye30LjDbC1xRVS91+7kSuBs4DlhbVY8O+sVIkjSXTefq8YsPMnzzYba/DrjuIOMbgY2vanaSJOllfiKaJEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNMNqSJDXCaEuS1AijLUlSI4y2JEmNOGK0k6xNsjPJI1PGTkqyKckT3fcTu/Ek+XSSrUkeTvL2Kc9Z1W3/RJJVx+blSJI0d03nnfYtwIoDxq4GNlfVMmBzdx/gfGBZ97UauAl6kQeuBc4GzgKu3Rd6SZI0PUeMdlXdA+w6YHglcGt3+1bgwinjt1XPfcCiJKcA7wI2VdWuqnoe2MQr/yIgSZIOY8EMnzdWVU93t78DjHW3FwNPTdluezd2qPFXSLKa3rt0xsbGmJycnOEUD27s9XDVGXsHus9BGvTrPZw9e/YM9XiznevRz/XYz7Xo53r0G+Z6zDTaL6uqSlKDmEy3vzXAGoDx8fGamJgY1K4BuHHdndyw5ahf9jGz7ZKJoR1rcnKSQa9vy1yPfq7Hfq5FP9ej3zDXY6ZXjz/Tnfam+76zG98BnDpluyXd2KHGJUnSNM002huAfVeArwLunDJ+WXcV+TnA7u40+t3AeUlO7C5AO68bkyRJ03TE88RJvgBMACcn2U7vKvDrgduTXA48Cby323wjcAGwFXgBeD9AVe1K8lHg/m67j1TVgRe3SZKkwzhitKvq4kM89M6DbFvAFYfYz1pg7auanSRJepmfiCZJUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktSIo4p2km1JtiR5KMkD3dhJSTYleaL7fmI3niSfTrI1ycNJ3j6IFyBJ0nwxiHfay6vqzKoa7+5fDWyuqmXA5u4+wPnAsu5rNXDTAI4tSdK8cSxOj68Ebu1u3wpcOGX8tuq5D1iU5JRjcHxJkuakVNXMn5x8C3geKOB3q2pNku9W1aLu8QDPV9WiJHcB11fVvd1jm4HfqKoHDtjnanrvxBkbG3vH+vXrZzy/g9m5azfPvDjQXQ7UGYvfOLRj7dmzh4ULFw7teLOd69HP9djPtejnevQb9HosX778wSlnr/ssOMp9/8Oq2pHk7wCbknx96oNVVUle1d8KqmoNsAZgfHy8JiYmjnKK/W5cdyc3bDnal33sbLtkYmjHmpycZNDr2zLXo5/rsZ9r0c/16DfM9Tiq0+NVtaP7vhP4MnAW8My+097d953d5juAU6c8fUk3JkmSpmHG0U5yfJIT9t0GzgMeATYAq7rNVgF3drc3AJd1V5GfA+yuqqdnPHNJkuaZozlPPAZ8ufdjaxYAf1BVX0lyP3B7ksuBJ4H3dttvBC4AtgIvAO8/imNLkjTvzDjaVfVN4GcPMv5XwDsPMl7AFTM9niRJ852fiCZJUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiOMtiRJjTDakiQ1wmhLktQIoy1JUiMWjHoC6rf06j8c2rGuOmMv75vB8bZd/+5jMBtJ0pH4TluSpEYYbUmSGmG0JUlqhNGWJKkRRluSpEYYbUmSGmG0JUlqhNGWJKkRRluSpEb4iWh61Yb5qW0z4Se2SZqrhv5OO8mKJN9IsjXJ1cM+viRJrRpqtJMcB3wWOB84Hbg4yenDnIMkSa0a9unxs4CtVfVNgCTrgZXAY0Oeh+awmZ6+n+kvUJmrbllx/KinIOkAw472YuCpKfe3A2cPeQ6SpmHLjt2z+i8xXrug+WjWXYiWZDWwuru7J8k3BnyIk4HnBrzPJn3QtejjevSb7euR/zzUw83qtRgB16PfoNfjJw71wLCjvQM4dcr9Jd3Yy6pqDbDmWE0gyQNVNX6s9t8S16Kf69HP9djPtejnevQb5noM++rx+4FlSU5L8lrgImDDkOcgSVKThvpOu6r2JrkSuBs4DlhbVY8Ocw6SJLVq6D/TrqqNwMZhH3eKY3bqvUGuRT/Xo5/rsZ9r0c/16De09UhVDetYkiTpKPjZ45IkNWLeRNuPT90vyalJvprksSSPJvnQqOc0akmOS/J/ktw16rmMWpJFSb6Y5OtJHk/yc6Oe0ygl+fXuz8kjSb6Q5HWjntMwJVmbZGeSR6aMnZRkU5Inuu8njnKOw3KItfgv3Z+Vh5N8OcmiYzmHeRFtPz71FfYCV1XV6cA5wBXzfD0APgQ8PupJzBKfAr5SVT8F/CzzeF2SLAY+CIxX1VvpXUB70WhnNXS3ACsOGLsa2FxVy4DN3f354BZeuRabgLdW1c8AfwFccywnMC+izZSPT62qHwL7Pj51Xqqqp6vqa93t79P7n/Li0c5qdJIsAd4NfH7Ucxm1JG8Efh64GaCqflhV3x3ppEZvAfD6JAuANwB/OeL5DFVV3QPsOmB4JXBrd/tW4MJhzmlUDrYWVfXHVbW3u3sfvc8fOWbmS7QP9vGp8zZSUyVZCrwN+LMRT2WUPgn8O+BvRjyP2eA04Fng97ofF3w+ybz9EPKq2gF8HPg28DSwu6r+eLSzmhXGqurp7vZ3gLFRTmYW+QDwR8fyAPMl2jqIJAuBLwG/VlXfG/V8RiHJLwI7q+rBUc9lllgAvB24qareBvyA+XPq8xW6n9WupPeXmTcBxyf5F6Od1exSvX+CNO//GVKSf0/vR4/rjuVx5ku0j/jxqfNNktfQC/a6qrpj1PMZoXOB9yTZRu/HJv84ye+PdkojtR3YXlX7zrx8kV7E56t/Anyrqp6tqh8BdwD/YMRzmg2eSXIKQPd954jnM1JJ3gf8InBJHeN/Rz1fou3Hp06RJPR+Zvl4VX1i1PMZpaq6pqqWVNVSev9d/I+qmrfvpKrqO8BTSd7cDb2T+f2rc78NnJPkDd2fm3cyjy/Mm2IDsKq7vQq4c4RzGakkK+j9eO09VfXCsT7evIh2d5HAvo9PfRy4fZ5/fOq5wKX03lU+1H1dMOpJadb4N8C6JA8DZwL/cbTTGZ3ujMMXga8BW+j9P3NefRpYki8A/xt4c5LtSS4Hrgd+IckT9M5GXD/KOQ7LIdbiM8AJwKbu/6X/9ZjOwU9EkySpDfPinbYkSXOB0ZYkqRFGW5KkRhhtSZIaYbQlSWqE0ZYkqRFGW5KkRhhtSZIa8f8Bc30vXmMrEnoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 频数直方图\n",
    "crime1=pd.read_stata('./data/crime1.dta')\n",
    "crime1.hist(column='narr86',figsize=(8,6))\n",
    "print(crime1.narr86.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，如果将其作为因变量$y$构建模型，会发生什么后果呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 违背正态性假设的后果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道，在MLR.1-MLR.5下，OLS估计具有最优线性无偏估计的性质，这一性质不需要满足正态性假设MLR.6，因此**违背正态性假设不会影响OLS系数估计的准确性与稳定性**。\n",
    "\n",
    "但是我们在进行假设检验的时候，基于t统计量与F统计量的推断是以满足MLR.6为前提的，这是否意味着对于narr86这种因变量而言，我们无法对其进行t检验与F检验呢？幸运的是，回答是否定的。得益于**中心极限定理**，我们有下述定理成立\n",
    "\n",
    "**· 定理：** 在MLR.1-MLR.5假定下，OLS估计系数的抽样分布满足\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)} \\Rightarrow N(0,1)\n",
    "$$\n",
    "即，OLS的系数估计满足渐进正态性，这意味着在大样本容量下，OLS估计系数是近似正态分布的。\n",
    "\n",
    "接下来的问题是，大样本下OLS估计是渐进正态分布的，按理来说应当使用标准正态分布作参考而不是用t分布作参考。但从实证角度来说，使用t分布进行大样本假设检验也是合理的，即以下写法是合理的\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_j-\\beta _j}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}\\Rightarrow t_{n-k-1}\n",
    "$$\n",
    "这是因为，当t分布自由度df达30以上时，就已经接近为正态分布了，而大样本下样本容量$n$的大数值将让自由度$n-k-1$也达到一个很大的数值，因此使用t分布进行检验实际上是没问题的！\n",
    "\n",
    "最后的问题是，样本容量究竟要有多大才算是大样本呢？这没有一个固定的答案，部分学者认为大于等于30就可以了，当然样本量越多越好这是毋庸置疑的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 异方差的分析——违反MLR.5的后果是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 如何理解与观测异方差性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先回顾一下同方差假设MLR.5：随机误差$u$的条件方差恒为一个常数，即\n",
    "$$\n",
    "\\operatorname{Var}\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}\n",
    "$$\n",
    "这个假设还可以拓展成因变量$y$的同方差性\n",
    "$$\n",
    "\\operatorname{Var}\\left(y \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}\n",
    "$$\n",
    "即，对于给定样本的$y_1,y_2,…,y_n$，**在每一个自变量$x_j$测度下**，它们的方差都应当是恒定的。这句话可以理解成，因变量/随机误差的方差在每一个自变量$x_j$下都不随自变量的变化而变化；而只要其在某一个自变量$x_j$下的方差是不稳定的，则同方差假设就不成立！\n",
    "\n",
    "这些文字看起来有点抽象，我们依旧一个例子向大家展示两种异方差的观测方法。\n",
    "\n",
    "**· Example12.** 我们研究住房价格price的影响因素，并初步确定模型为\n",
    "$$\n",
    "\\text { price }=\\beta_{0}+\\beta_{1} \\text { lotsize }+\\beta_{2} \\text { sqrft }+\\beta_{3} b d r m s+u\n",
    "$$\n",
    "即，我们纳入了三个自变量：lotsize,sqrft,bdrms。要想观测样本是否存在异方差性，第一种方法就是直接查看因变量$y$分别在三个自变量下的散点分布图，看看因变量在自变量值变化时的散布特征是否不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'price')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用数据集hprice1，前面已经加载过\n",
    "fig=plt.figure(figsize=(13,6))\n",
    "ax1=fig.add_subplot(1,3,1)\n",
    "hprice2=hprice1[hprice1['lotsize']<80000] #去掉了一个极端点，方便观察\n",
    "plt.scatter(hprice2.lotsize,hprice2.price,axes=ax1)\n",
    "ax1.set_xlabel('lotsize')\n",
    "ax1.set_ylabel('price')\n",
    "\n",
    "ax2=fig.add_subplot(1,3,2)\n",
    "plt.scatter(hprice1.sqrft,hprice1.price,axes=ax2)\n",
    "ax2.set_xlabel('sqrft')\n",
    "ax2.set_ylabel('price')\n",
    "\n",
    "ax3=fig.add_subplot(1,3,3)\n",
    "plt.scatter(hprice1.bdrms,hprice1.price,axes=ax3)\n",
    "ax3.set_xlabel('bdrms')\n",
    "ax3.set_ylabel('price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，lotsize与sqrft的散布情况相对稳定，异方差的情况并不明显；而bdrms就不一样了，当$bdrms=3$时，因变量值分布呈现出往较小值偏移的特征，表现为上面稀疏，下面稠密。而$bdrms=4$时，因变量值的分布则相对对称，这说明price在bdrms上存在异方差的现象！\n",
    "\n",
    "另一种观测异方差的方法是先进行OLS拟合，并以0为参考基准，观测残差在三个自变量上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'resid')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hprice1_lm=sm.formula.ols('price~lotsize+sqrft+bdrms',data=hprice1).fit()\n",
    "hprice1['resid']=hprice1_lm.resid\n",
    "\n",
    "# 画图\n",
    "fig=plt.figure(figsize=(13,6))\n",
    "ax1=fig.add_subplot(1,3,1)\n",
    "hprice3=hprice1[hprice1['lotsize']<80000] #去掉了一个极端点，方便观察\n",
    "plt.scatter(hprice3.lotsize,hprice3.resid,axes=ax1)\n",
    "ax1.set_xlabel('lotsize')\n",
    "ax1.set_ylabel('resid')\n",
    "\n",
    "ax2=fig.add_subplot(1,3,2)\n",
    "plt.scatter(hprice1.sqrft,hprice1.resid,axes=ax2)\n",
    "ax2.set_xlabel('sqrft')\n",
    "ax2.set_ylabel('resid')\n",
    "\n",
    "ax3=fig.add_subplot(1,3,3)\n",
    "plt.scatter(hprice1.bdrms,hprice1.resid,axes=ax3)\n",
    "ax3.set_xlabel('bdrms')\n",
    "ax3.set_ylabel('resid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用残差观测的结果得出的结论也是相似的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 违背同方差假设的后果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型形式误设（导致违背MLR.1）以及随机误差的非正态性对模型的影响并不明显，而违反同方差假设的后果就相对严重了:\n",
    "\n",
    "1. 首先由于MLR.1-MLR.4仍然成立，因此系数估计的无偏性依然成立。\n",
    "2. 异方差下，OLS估计不再是**最优**线性无偏估计。\n",
    "3. 异方差下，估计系数的方差计算不再准确，从而影响到标准误的计算，进而影响到t检验与F检验的有效性。\n",
    "\n",
    "我们注意第三点，并不是说在异方差下我们原先的t检验统计量不再服从t分布了，而是因为t检验统计量中含有标准误，而标准误如果在异方差条件下按照同方差的方式计算，会导致t检验统计量不准确，而如果我们可以将之纠正，我们依旧可以进行t检验与F检验！\n",
    "\n",
    "有关异方差的解决方法，我们将在下一章节介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 异方差下的回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在上一小节已经知道了异方差导致的两个后果：OLS不再最优、t检验与F检验不再稳健。那么在这一节我们就要针对这两个问题提出解决的方法。\n",
    "\n",
    "在这一章节我们将学习：\n",
    "\n",
    "1. 在异方差下如何重新估计标准差/标准误，并进行稳健的推断（假设检验）\n",
    "2. 如何用假设检验检测数据的异方差性\n",
    "3. 如何修正OLS估计方法达到最优（广义OLS法-GLS）\n",
    "\n",
    "我们要分清1与3的区别：方法1仍然建立在OLS估计上，只不过采取了一些措施使得假设检验变得稳健；方法3则是将OLS改进为一种新的估计方法GLS。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 异方差稳健的t检验与F检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓稳健方法，就是这种方法不论是在异方差还是同方差下都可以进行使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 重新估计方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们回顾一下在同方差假设下$\\hat{\\beta_j}$的方差估计：\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)=\\frac{\\sigma^{2}}{\\sum_{i=j}^{n} \\hat{r}_{i j}^{2}}=\\frac{\\sigma^{2}}{S S T_{j}\\left(1-R_{j}^{2}\\right)}\n",
    "$$\n",
    "事实上，这个方差估计有一个更广义的表达\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)=\\frac{\\sum \\hat{r}_{i j}^{2} \\sigma_{i}^{2}}{R S S_{j}^{2}}\n",
    "$$\n",
    "在同方差假设下，由于$\\sigma_{i}=\\sigma$，才有了上面化简了的表达式。然而，在异方差下，我们不能简单地认为$\\sigma_{i}=\\sigma$，而是对于每一个$\\sigma_{i}$都要有一个对应的估计$\\hat{\\sigma _i}$。那么，对于$\\hat{\\sigma _i}$，我们要怎么估计它呢？答案是，直接用残差$\\hat{u_i}$代替，即$\\hat{\\sigma _i}=\\hat{u_i}$，系数重新估计的方差就变为了\n",
    "$$\n",
    "\\widehat{\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)}=\\frac{\\sum \\hat{r}_{i j}^{2} \\hat{u}_{i}^{2}}{R S S_{j}^{2}}\n",
    "$$\n",
    "至此，我们就可以利用新的估计方差进行稳健t检验与F检验了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 稳健t检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于单参数检验问题，我们使用稳健t检验。\n",
    "\n",
    "先看单参数检验问题\n",
    "$$\n",
    "H_{0}: \\beta_{j}=\\beta_{j0} \\leftrightarrow H_{1}: \\beta_{j} \\neq \\beta_{j 0}\n",
    "$$\n",
    "定义t统计量\n",
    "$$\n",
    "t=\\frac{\\hat{\\beta}_j-\\beta _{j0}}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}\\approx N\\left( 0,1 \\right) \n",
    "$$\n",
    "注意，此时t统计量不再服从t分布，而是服从标准正态分布！（因此需要用到正态分布的p值和分位点，除此以外检验的原理完全不变）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "同方差假设下模型的方差估计与推断结果：\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -21.7703     29.475     -0.739      0.462     -80.385      36.844\n",
      "lotsize        0.0021      0.001      3.220      0.002       0.001       0.003\n",
      "sqrft          0.1228      0.013      9.275      0.000       0.096       0.149\n",
      "bdrms         13.8525      9.010      1.537      0.128      -4.065      31.770\n",
      "==============================================================================\n",
      "-------------------------------------------\n",
      "异方差假设下模型的稳健方差估计与推断结果：\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -21.7703     36.284     -0.600      0.549     -92.886      49.346\n",
      "lotsize        0.0021      0.001      1.691      0.091      -0.000       0.004\n",
      "sqrft          0.1228      0.017      7.090      0.000       0.089       0.157\n",
      "bdrms         13.8525      8.284      1.672      0.094      -2.383      30.088\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# 以前面的price1_lm为例\n",
    "# hprice1_lm=sm.formula.ols('price~lotsize+sqrft+bdrms',data=hprice1).fit()\n",
    "print('同方差假设下模型的方差估计与推断结果：')\n",
    "print(hprice1_lm.summary().tables[1])\n",
    "print('-------------------------------------------')\n",
    "hprice1_lm_1=sm.formula.ols('price~lotsize+sqrft+bdrms',data=hprice1).fit(cov_type='HC0', use_t=False)\n",
    "# use_t：稳健t检验是否使用t分布进行推断，False表示拒绝使用t分布，改用标准正态分布\n",
    "print('异方差假设下模型的稳健方差估计与推断结果：')\n",
    "print(hprice1_lm_1.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上结果展示了异方差稳健t检验的结果与普通t检验的结果确实会有不同：lotsize在普通t检验下是显著的，但在稳健t检验中是不显著的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 稳健F检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多参数的联合检验，我们使用稳健F检验。\n",
    "\n",
    "稳健F检验的问题设置与原来的F检验一样，且它们的F检验统计量也完全一样，即\n",
    "$$\n",
    "F=\\frac{\\left(R S S_{r}-R S S_{u r}\\right) / q}{R S S_{u r} /(n-k-1)}\n",
    "$$\n",
    "只不过，在稳健检验下，F统计量并不严格服从F分布，而是渐进服从自由度为$q$的卡方分布。\n",
    "\n",
    "我们考虑下述模型\n",
    "$$\n",
    "cumgpa=\\beta _0+\\beta _1sat+\\beta _2hsperc+\\beta _3tothrs+\\beta _4female+\\beta _5black+\\beta _6white+u\n",
    "$$\n",
    "并做假设检验\n",
    "$$\n",
    "H_0: \\beta _5=\\beta _6=0 \\leftrightarrow \\,\\,H_1: H_0\\text{不成立}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "gpa3=pd.read_stata('./data/gpa3.dta')\n",
    "gpa3=gpa3[gpa3['term']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<F test: F=array([[0.74779698]]), p=0.47414427215551025, df_denom=359, df_num=2>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接训练无约束模型，并采用稳健估计训练\n",
    "gpa3_lm=sm.formula.ols('cumgpa~sat+hsperc+tothrs+female+black+white',data=gpa3).fit(cov_type='HC0')\n",
    "\n",
    "hypotheses='(black=0),(white=0)' # 另一种做联合检验的形式，直接将原假设写出来\n",
    "gpa3_lm.wald_test(hypotheses,use_f=True,scalar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的报告中，F值为0.74779698，p值为0.47414427215551025，显然，不能拒绝原假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 异方差的诊断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管异方差稳健的t、F检验适用范围更广，但是如果数据不存在异方差，则传统OLS估计及其假设检验依然是最优的；此外，如果我们能确定存在异方差且可以估计出异方差的形式，那么我们可以获得比OLS估计更好的估计方法。而它们的前提都是我们可以准确地判断出数据是否存在异方差。\n",
    "\n",
    "在前面的学习中，我们通过散点图的方法粗略地判断模型是否存在异方差，但是这样的判断方法过于主观，我们需要一种可靠客观的诊断方法。\n",
    "\n",
    "异方差检验的种类非常多，有的检验只检验方差是否“恒定”，而不检验方差是否与自变量无关，即方差是某些自变量的函数$\\sigma =\\sigma \\left( x \\right)$。事实上，比起纯粹地知晓方差是否恒定，我们更关心后者，这是因为如果方差确实是某个自变量的函数，那么我们就可以采用广义OLS估计法（也就是GLS）进行更有效地估计。\n",
    "\n",
    "关于检验方差是否与自变量无关的方法，最常用的便是BP异方差检验与White异方差检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 BP异方差检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 检验思想**\n",
    "\n",
    "我们取原假设为：假定同方差假设正确，即\n",
    "$$\n",
    "\\operatorname{Var}\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}\n",
    "$$\n",
    "由于MLR.2假设：$E\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=0$的成立，则同方差假设等价为\n",
    "$$\n",
    "E\\left( u^2\\mid x_1,\\cdots ,x_k \\right) =\\sigma ^2\n",
    "$$\n",
    "这个假设说明的是$u^2$的条件均值应当为一个常数而不是一个与自变量$x_j$有关的函数。我们在前面提到过，回归的本质是条件均值建模，而这个假设刚好就是有关$u^2$条件均值的假设，于是我们可以**根据我们实际要检验的问题构建线性回归模型**，并将检验问题等价为模型系数的显著性检验。\n",
    "\n",
    "BP检验的实质是检验异方差与所有自变量的一次项无关，这就等价于对一个以$u^2$为因变量，以$x_j$为自变量的线性回归模型\n",
    "$$\n",
    "u^{2}=\\delta_{0}+\\delta_{1} x_{1}+\\delta_{2} x_{2}+\\cdots+\\delta_{k} x_{k}+v\n",
    "$$\n",
    "做全部变量系数的联合显著性检验，于是原假设变为\n",
    "$$\n",
    "\\mathrm{H}_{0}: \\delta_{1}=\\delta_{2}=\\cdots=\\delta_{k}=0\n",
    "$$\n",
    "如果原假设被拒绝，就意味着存在一种与某自变量一次项相关的异方差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· BP检验的步骤（了解即可）**\n",
    "\n",
    "1. 使用OLS法估计模型\n",
    "$$\n",
    "y \\sim x_{1}+x_{2}+\\cdots+x_{k-1}+x_{k}\n",
    "$$\n",
    "获得残差的平方$\\hat{u}^2$\n",
    "\n",
    "2. 使用OLS估计法估计模型\n",
    "$$\n",
    "\\hat{u}^2 \\sim x_{1}+x_{2}+\\cdots+x_{k-1}+x_{k}\n",
    "$$\n",
    "\n",
    "3. 对所有变量系数做联合检验，可用F检验或大样本LM检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 用python实现BP检验**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依旧以前面的hprice1_lm为例，检验price~lotsize+sqrft+bdrms是否存在异方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.092385504350233,\n",
       " 0.0027820595556890993,\n",
       " 5.338919363241416,\n",
       " 0.0020477444209360787)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# 直接输出\n",
    "# hprice1_lm=sm.formula.ols('price~lotsize+sqrft+bdrms',data=hprice1).fit()\n",
    "hprice1_lm_reg=sm.formula.ols('price~lotsize+sqrft+bdrms',data=hprice1) # 相比于hprice1_lm，他没有fit()\n",
    "het_breuschpagan(hprice1_lm.resid,hprice1_lm_reg.exog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bp_lm_statistic    14.092386\n",
       "bp_lm_pval          0.002782\n",
       "bp_F_statistic      5.338919\n",
       "bp_F_pval           0.002048\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个输出bp检验的函数\n",
    "def bp_test(res, X):\n",
    "    result_bp_test = sm.stats.diagnostic.het_breuschpagan(res, X)\n",
    "    bp_lm_statistic = result_bp_test[0]\n",
    "    bp_lm_pval = result_bp_test[1]\n",
    "    bp_F_statistic= result_bp_test[2]\n",
    "    bp_F_pval = result_bp_test[3]\n",
    "    bp_test_output=pd.Series(result_bp_test[0:4],index=['bp_lm_statistic','bp_lm_pval','bp_F_statistic','bp_F_pval'])    \n",
    "    return bp_test_output\n",
    "\n",
    "bp_test(hprice1_lm.resid,hprice1_lm_reg.exog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们猜想的那样，确实存在异方差现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 White异方差检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP检验考虑的是是否存在与变量的一次项相关的异方差，而White检验则增加了对所有变量二次项以及交互项的考察。\n",
    "\n",
    "假若模型包含三个变量，BP检验所检验的模型是\n",
    "$$\n",
    "y\\sim x_1+x_2+x_3\n",
    "$$\n",
    "而White检验所检验的模型是\n",
    "$$\n",
    "y\\sim x_1+x_2+x_3+{x_1}^2+{x_2}^2+{x_3}^2+x_1x_2+x_1x_3+x_2x_3\n",
    "$$\n",
    "White检验虽然考虑的问题范围更广，但是由于回归元个数指数级增长，模型自由度相比于BP检验大大降低，对于自变量较多的情况，White检验可能并不如BP检验合适。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white_lm_statistic    33.731658\n",
       "white_lm_pval          0.000100\n",
       "white_F_statistic      5.386953\n",
       "white_F_pval           0.000010\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "# White检验函数在python上的使用与bp检验完全一样\n",
    "\n",
    "def white_test(res, X):\n",
    "    result_bp_test = sm.stats.diagnostic.het_white(res, X)\n",
    "    bp_lm_statistic = result_bp_test[0]\n",
    "    bp_lm_pval = result_bp_test[1]\n",
    "    bp_F_statistic= result_bp_test[2]\n",
    "    bp_F_pval = result_bp_test[3]\n",
    "    white_test_output=pd.Series(result_bp_test[0:4],index=['white_lm_statistic','white_lm_pval','white_F_statistic','white_F_pval'])    \n",
    "    return white_test_output\n",
    "    \n",
    "white_test(hprice1_lm.resid,hprice1_lm_reg.exog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 广义最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归存在异方差的另一种解决方法，就是根据异方差的具体形式使用广义的最小二乘法，对模型进行重新估计。对于这种解决方法，变量系数的估计也会发生变化，但是在异方差情形下，它比传统的OLS估计法更优！\n",
    "\n",
    "我们提到“根据异方差的具体形式”，是指异方差可以用自变量的函数被表达出来。如果它能被我们找出来，我们就可以是加权最小二乘估计WLS；如果由于函数形式复杂而无法被判断出来，我们则使用可行的广义最小二乘估计FGLS。\n",
    "\n",
    "接下来我们就来学习一下这两种方法吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 加权最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加权最小二乘法的原理非常简单。\n",
    "\n",
    "假设异方差的形式除去一个常数外是已知的，即\n",
    "$$\n",
    "\\operatorname{Var}(u \\mid x)=\\sigma^{2} h(x)\n",
    "$$\n",
    "那么如果我们在原模型两边同除以$\\frac{1}{\\sqrt{h\\left( x \\right)}}$，即\n",
    "$$\n",
    "\\frac{y}{\\sqrt{h\\left( x \\right)}}=\\frac{\\beta _{0}^{*}}{\\sqrt{h\\left( x \\right)}}+\\beta _{1}^{*}\\frac{x_1}{\\sqrt{h\\left( x \\right)}}+\\cdots +\\beta _{k}^{*}\\frac{x_k}{\\sqrt{h\\left( x \\right)}}+\\frac{u}{\\sqrt{h\\left( x \\right)}}\n",
    "$$\n",
    "\n",
    "并将带有$\\frac{1}{\\sqrt{h\\left( x \\right)}}$的自变量与随机误差视作是新的变量与随机误差，则问题就是同方差情形了，我们可以使用OLS估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用一个简单的例子教会大家如何鉴别简单的异方差形式，并用statsmodels包中的wls函数进行wls估计。\n",
    "\n",
    "**· Example13.** 我们想研究27家企业主管人数Y与工人人数X的关系。由于只有一个自变量，一开始我们可以考虑简单线性回归模型\n",
    "$$\n",
    "Y=\\beta _0+\\beta _1X+u\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>247</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>267</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X   Y\n",
       "0  294  30\n",
       "1  247  32\n",
       "2  267  37\n",
       "3  358  44\n",
       "4  423  47"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入数据集\n",
    "data=pd.read_table('./data/P176.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在判断异方差函数的形式上，可视化发挥着重要的作用。我们可以采用之前画散点图的办法，初步观测样本分布的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp_lm_statistic    16.028369\n",
      "bp_lm_pval          0.000062\n",
      "bp_F_statistic     36.522300\n",
      "bp_F_pval           0.000003\n",
      "dtype: float64\n",
      "----------------------------------\n",
      "white_lm_statistic    2.113448e+01\n",
      "white_lm_pval         2.574574e-05\n",
      "white_F_statistic     4.323809e+01\n",
      "white_F_pval          1.104869e-08\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 直接看Y与X的散点图\n",
    "fig=plt.figure(figsize=(13,6))\n",
    "ax1=fig.add_subplot(1,2,1)\n",
    "plt.scatter(data.X,data.Y,axes=ax1)\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_title('Y | X')\n",
    "\n",
    "# 查看ols估计的残差与X的散点图\n",
    "data_lm=sm.formula.ols('Y~X',data=data).fit()\n",
    "ax2=fig.add_subplot(1,2,2)\n",
    "plt.scatter(data.X,data_lm.resid,axes=ax2)\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('resid_ols')\n",
    "ax2.set_title('resid_ols | X')\n",
    "\n",
    "# 使用BP检验\n",
    "data_lm_reg=sm.formula.ols('Y~X',data=data)\n",
    "print(bp_test(data_lm.resid,data_lm_reg.exog))\n",
    "print('----------------------------------')\n",
    "# 使用White检验\n",
    "data_lm_reg=sm.formula.ols('Y~X',data=data)\n",
    "print(white_test(data_lm.resid,data_lm_reg.exog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bp检验与White检验均表示，回归确实存在明显的异方差线性。且根据散点图，我们可以看到随着X的增大，样本的分布越来越分散，即方差越来越大。我们设想：方差可能与X的二次项成正比（一次项也有可能，言之有理即可），于是我们假定\n",
    "$$\n",
    "\\operatorname{Var}(u \\mid x)=\\sigma^{2} x^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     14.4481      9.562      1.511      0.143      -5.245      34.141\n",
      "X              0.1054      0.011      9.303      0.000       0.082       0.129\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      3.8033      4.570      0.832      0.413      -5.608      13.215\n",
      "X              0.1210      0.009     13.445      0.000       0.102       0.140\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "data_lm_wls=sm.formula.wls('Y~X',weights=1/data.X**2,data=data).fit()\n",
    "# 注意：weights传入的是一个数组，不是一个“表达式”。如果方差函数为h(x)，则要传入1/h(x)的数组\n",
    "print(data_lm.summary().tables[1])\n",
    "print(data_lm_wls.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对比一下两种估计方法的结果。加入我们对方差的假设是正确的，wls估计的系数标准误要小于ols估计的系数标准误，这说明wls在异方差下表现优于ols估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 可行的广义最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WLS估计法要求我们知晓$h(x)$的具体形式，而在大多数情况下，$h(x)$的形式是难以通过观察得出的，这个时候我们就需要使用一种方法估计出$h(x)$的形式，再使用wls估计法求解模型。这种方法又称为FGLS。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 如何估计$h(x)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FGLS将$h(x)$假设为\n",
    "$$\n",
    "h(x)=\\exp \\left(\\delta_{0}+\\delta_{1} x_{1}+\\cdots+\\delta_{k} x_{k}\\right)\n",
    "$$\n",
    "则条件方差的表达式就有\n",
    "$$\n",
    "\\mathrm{Var(}u\\mid x)=E\\left( u^2\\mid x \\right) =\\sigma ^2\\exp \\left( \\delta _0+\\delta _1x_1+\\cdots +\\delta _kx_k \\right) \n",
    "$$\n",
    "进而有\n",
    "$$\n",
    "E\\left( u^2\\mid x \\right) =\\sigma ^2\\exp \\left( \\delta _0+\\delta _1x_1+\\cdots +\\delta _kx_k \\right) \\Rightarrow \\log \\left( u^2 \\right) \\sim x_1+\\cdots +x_k\n",
    "$$\n",
    "也就是说，我们先用OLS估计求解一个以$\\log \\left(\\hat{u}^{2}\\right)$为因变量的线性回归模型，再将该模型进行指数化处理，便可以得到$h(x)$的估计形式了。\n",
    "\n",
    "我们总结出以下步骤：\n",
    "\n",
    "1. 做回归$y \\sim x_{1}+\\cdots x_{k}$，得到残差$\\hat{u}$\n",
    "2. 做回归$\\log \\left(\\hat{u}^{2}\\right) \\sim x_{1}+\\cdots+x_{k}$，得到拟合值$\\hat{g}$\n",
    "3. 计算函数$\\hat{h}=\\exp (\\hat{g})$\n",
    "4. 以$1/\\hat{h}$做权重，用wls估计模型$y \\sim x_{1}+\\cdots x_{k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 用 python实现FGLS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statsmodels没有直接进行FGLS的函数，我们需要手动估计$h(x)$。我们考虑下面的模型，用FGLS对其进行估计\n",
    "$$\n",
    "\\text { cigs }=\\beta_{0}+\\beta_{1} \\log (\\text { income })+\\beta_{2} \\log (\\text { cigpric })+\\beta_{3} \\text { educ }+\\beta_{4} a g e+\\beta_{5} \\text { age }{ }^{2}+\\beta_{6} \\text { restaurn }+u\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>educ</th>\n",
       "      <th>cigpric</th>\n",
       "      <th>white</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>cigs</th>\n",
       "      <th>restaurn</th>\n",
       "      <th>lincome</th>\n",
       "      <th>agesq</th>\n",
       "      <th>lcigpric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>60.506001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.903487</td>\n",
       "      <td>2116.0</td>\n",
       "      <td>4.102743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>57.882999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.308950</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>4.058424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>57.664001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.308950</td>\n",
       "      <td>3364.0</td>\n",
       "      <td>4.054633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.5</td>\n",
       "      <td>57.882999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.903487</td>\n",
       "      <td>900.0</td>\n",
       "      <td>4.058424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>58.320000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.903487</td>\n",
       "      <td>289.0</td>\n",
       "      <td>4.065945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   educ    cigpric  white   age   income  cigs  restaurn    lincome   agesq  \\\n",
       "0  16.0  60.506001    1.0  46.0  20000.0   0.0       0.0   9.903487  2116.0   \n",
       "1  16.0  57.882999    1.0  40.0  30000.0   0.0       0.0  10.308950  1600.0   \n",
       "2  12.0  57.664001    1.0  58.0  30000.0   3.0       0.0  10.308950  3364.0   \n",
       "3  13.5  57.882999    1.0  30.0  20000.0   0.0       0.0   9.903487   900.0   \n",
       "4  10.0  58.320000    1.0  17.0  20000.0   0.0       0.0   9.903487   289.0   \n",
       "\n",
       "   lcigpric  \n",
       "0  4.102743  \n",
       "1  4.058424  \n",
       "2  4.054633  \n",
       "3  4.058424  \n",
       "4  4.065945  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoke=pd.read_stata('./data/smoke.dta')\n",
    "smoke.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   cigs   R-squared:                       0.113\n",
      "Model:                            WLS   Adj. R-squared:                  0.107\n",
      "Method:                 Least Squares   F-statistic:                     17.06\n",
      "Date:                Wed, 13 Jul 2022   Prob (F-statistic):           1.32e-18\n",
      "Time:                        15:48:06   Log-Likelihood:                -3207.8\n",
      "No. Observations:                 807   AIC:                             6430.\n",
      "Df Residuals:                     800   BIC:                             6462.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        5.6355     17.803      0.317      0.752     -29.311      40.582\n",
      "log(income)      1.2952      0.437      2.964      0.003       0.437       2.153\n",
      "log(cigpric)    -2.9403      4.460     -0.659      0.510     -11.695       5.815\n",
      "educ            -0.4634      0.120     -3.857      0.000      -0.699      -0.228\n",
      "age              0.4819      0.097      4.978      0.000       0.292       0.672\n",
      "I(age ** 2)     -0.0056      0.001     -5.990      0.000      -0.007      -0.004\n",
      "restaurn        -3.4611      0.796     -4.351      0.000      -5.023      -1.900\n",
      "==============================================================================\n",
      "Omnibus:                      325.055   Durbin-Watson:                   2.050\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1258.138\n",
      "Skew:                           1.908   Prob(JB):                    6.29e-274\n",
      "Kurtosis:                       7.780   Cond. No.                     2.30e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.3e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# 先进行ols估计\n",
    "smoke_lm_ols=sm.formula.ols('cigs~log(income)+log(cigpric)+educ+age+I(age**2)+restaurn',data=smoke).fit()\n",
    "smoke['resid']=smoke_lm_ols.resid\n",
    "\n",
    "# 进行辅助回归\n",
    "smoke_lm_log=sm.formula.ols('log(resid**2)~log(income)+log(cigpric)+educ+age+I(age**2)+restaurn',data=smoke).fit()\n",
    "h_hat=np.exp(smoke_lm_log.fittedvalues)\n",
    "\n",
    "# 进行wls检验\n",
    "smoke_lm_wls=sm.formula.wls('cigs~log(income)+log(cigpric)+educ+age+I(age**2)+restaurn',weights=1/h_hat,data=smoke).fit()\n",
    "print(smoke_lm_wls.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业1：含二次项/对数项模型的讨论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们想要探究婴儿出生的体重与何种因素相关，数据集为bwght2.dta，本次习题所使用的变量解释如下：\n",
    "\n",
    "因变量：\n",
    "<br>\n",
    "· bwght：婴儿出生体重\n",
    "\n",
    "自变量：\n",
    "<br>\n",
    "· npvis：母亲产前检查次数\n",
    "<br>\n",
    "· mage：母亲年龄\n",
    "\n",
    "**使用python进行实操并回答以下问题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）：使用OLS估计方程\n",
    "$$\n",
    "\\log (b w g h t)=\\beta_{0}+\\beta_{1} n p v i s+\\beta_{2} n p v i s^{2}+u\n",
    "$$\n",
    "输出报告表，并回答：自变量npvis的二次项是否显著？自变量npvis是否对因变量有显著影响？\n",
    "\n",
    "（2）：基于（1）的方程，我们认为最大化log(bwght)的产前检查次数npvis约为24，其理论依据是什么？\n",
    "\n",
    "（3）：按照这个模型的结果，在24次产前检查后婴儿出生体重会下降，这是为什么？你认为这有实际意义吗？这蕴含了一个含二次项变量模型的常见陷阱，请仔细思考！\n",
    "\n",
    "（4）：在模型中加入母亲年龄变量及其二次形式。回答：保持npvis不变，母亲在什么生育年龄时，孩子出生体重最大？大于这个年龄时，孩子出生体重下降，这是否具有实际意义呢？请结合问题（3）思考这一问题。\n",
    "\n",
    "（5）：（4）中的模型能否解释log(gwght)大部分变异？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业2：异方差模型的讨论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "紧接着example13（主管人数与工人人数间关系的问题），我们对异方差问题进行更进一步的探讨。\n",
    "\n",
    "**使用python进行实操并回答以下问题**\n",
    "\n",
    "（1）：在假设方差形式为$\\operatorname{Var}(u \\mid x)=\\sigma^{2} x^{2}$并进行wls估计后，比较wls估计与ols估计的残差图，回答：异方差消除了吗？\n",
    "\n",
    "（2）：使用FGLS估计对该模型进行重新估计，观察残差图并回答：异方差消除了吗？\n",
    "\n",
    "（3）：画出log(Y)与X的散点图，观察方差的状况，说说你的发现；根据散点图的情况，请大胆假设一个你认为正确的模型。\n",
    "\n",
    "（4）：考虑新模型\n",
    "$$\n",
    "\\log (Y)=\\beta_{0}+\\beta_{1} X+\\beta_{2} X^2+u\n",
    "$$\n",
    "使用ols估计该模型，并画出残差散点图，说说你的发现\n",
    "\n",
    "（5）：综合以上四个问题，谈谈你对纠正模型异方差的见解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d04da679f0afee458c3e357f6d2cf382c79de022618e35f1262af9ecf478f5e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
